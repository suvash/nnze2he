{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7e54d1-6dfd-4955-ad85-51f214b84e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://paperswithcode.com/method/resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73bf3bb-aa64-414b-9359-f6272712ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c0ab78-3651-4a1d-b1ca-59dcf660cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image feeds into a resnet\n",
    "# Many many layers\n",
    "# with repeating structure all the way to predictions\n",
    "# the repeat struct is made up of block\n",
    "# the blocks are stacked up\n",
    "#\n",
    "# they're called bottleneck blocks\n",
    "# https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L108\n",
    "#\n",
    "# these have a init and forward pass similar to what we've done before for bn\n",
    "# https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L128-L141\n",
    "# https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L128-L141\n",
    "#\n",
    "# the example has convolutional layers\n",
    "# same thing as linear layers, except conv layers have spatial structure\n",
    "# the linear bias and multiplication is done in patches that walks across the image, instead of the full input\n",
    "# convolution basically do Wx + b on overlapping patches throughout the image\n",
    "# norm layer - is a batch norm 2d layer\n",
    "# relu and tanh - for very deep networks relu typically empirically works better\n",
    "# can see in the forward\n",
    "# conv - batch norm - relu - conv - batch norm - relu\n",
    "# in the end there's residual connection that we haven't talked about yet\n",
    "#\n",
    "# but the motif that you'll see in stacks is typical (notice that there's no bias if norm layer is present)\n",
    "# weights product layer - normalisation - non linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7b7fde2-9e9a-417a-a276-7472800cf9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haven't covered Convolutions yet\n",
    "# But conv.s are linear layers except on patches, that's really it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dcbb7a9-11ce-4ac2-b4a1-9c05b6872f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a010b8f4-0556-488a-be32-ccfc76a72e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A linear layers performs Wx + b\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "#\n",
    "# to initialize, need to know fan_in, fan_out\n",
    "# whether bias or not\n",
    "# outputs are weight and bias\n",
    "# also docs on how they're initialised\n",
    "# if the layers are init. properly, with roughly gaussian input, you'll have roughly gaussian output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b60d106d-01f1-494a-92b2-6c301ae3cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batchnorm layer\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "#\n",
    "# inputs are number of features\n",
    "# epsilon value (never divide by zero)\n",
    "# momentum (running mean, stddev) : may want to change, if the batch size is large, since the mean std dev might be fairly stable\n",
    "# if batch size is very small(say 32), momentum of 0.1 is potentially dangerous, as it might keep jumping and never converge\n",
    "# affine determines if the norm layers has affine learnable parameters, the gain and the bias, almost always kept to true\n",
    "# not sure why you want that to be false\n",
    "# track running stats determines whether the running mean and stddev is kept around\n",
    "# one reason to skip is if you want to estimate them at the end of training as stage 2\n",
    "# what device\n",
    "# datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb5e483-1145-4bd5-b03e-fc320c35de34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0c36e3-165b-4367-b10b-78cb61effe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of lecture\n",
    "# Importance of activations and gradients and their statistics\n",
    "# Especially important as you make your NNs larger and deeper\n",
    "#\n",
    "# Saw distributions at the output layer\n",
    "# Too confident mispredictions : very high loss - hockey stick loss curves\n",
    "# Fixing the init so everything is just as likely - spend more time on optimizing - no hockey stick - better losses at the end\n",
    "#\n",
    "# Control the activations\n",
    "# Don't want to squash to zero or explode to inifinity\n",
    "# can run into lot of trouble\n",
    "# want everything to be fairly homogenous throughout the NN\n",
    "# roughly gaussian activation throughout the NN\n",
    "# if we want roughly gaussian activation throughout the NN, how we init these weights and biases so everything is as well behaved\n",
    "# but that strategy is not possible for much deeper nets, hard to precisely init weights and biases for everything\n",
    "#\n",
    "# that gives way to normalisation layer, which normalises activation over a batch\n",
    "# take the activations over a batch, mean and std dev of that, then scale and shift the data (scale and shift is learable)\n",
    "# but since we are coupling all the elements in batches, how do we do the inference\n",
    "# introduce keeping track of running mean and std dev over the training set OR stage 2 calculation of mean std dev after training\n",
    "# use that later during inference\n",
    "#\n",
    "# No One likes this layer, causes a huge amount of bugs\n",
    "# Intuitively it's coupling what use to be a single forward pass, with a batch forward pass\n",
    "# Author has shot himself in the foot over and over with this layer\n",
    "# Avoid it as much as possible\n",
    "# Very influential when it came out - first time you could reliable train much deeper neural nets\n",
    "# The layer was very effective at controlling statistics\n",
    "# Some other alternatives are GroupNorm, LayerNorm, those have become more common in more recent deep learning\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f552d65d-fb18-48c2-8e9c-293cff9d9092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In future lessons, we will dive in Recurrent NNs\n",
    "# when you unroll the loop\n",
    "# all these analysis around activations and gradients will become very very important for good performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
