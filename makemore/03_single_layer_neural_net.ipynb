{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec678e4-77e9-4798-ae49-2201fb9e634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8d9cc3-00d1-454e-a7e9-f8edf008a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's only have one special token, and let's have it at index 0, offset others by 1\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "num_classes = len(stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6ae3a2-ccca-43a3-997d-0d7ca6855c60",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Time to make a training set of all the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eb4296f-b159-4284-8286-50fc23fe87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training set of bigrams (x,y)\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e79426-a45c-4e1e-88a8-45edeadbbe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edcc9a36-a456-4001-888c-b236827adf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beware of torch.tensor(autmatically guesses the dtype) vs. torch.Tensor(float dtype)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1186388a-59b1-4485-b908-16e32348b3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  5, 13, 13,  1]), tensor([ 5, 13, 13,  1,  0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d01721-70e3-4317-b29d-21faa398c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a common way of encoding integers is called one-hot encoding\n",
    "# neural networks shouldn't take in integers, instead they should take float values\n",
    "# pytorch has a built-in to one hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ac8047-1faa-4bcf-b5e2-6a4f5c94b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8632781a-c6d7-4df6-b2c5-fe377fb71bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=num_classes).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db355ab-194f-4faa-92ef-bd3f6a399fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 27]), torch.float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape, xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75140a1e-1c43-4723-ae11-a5593890227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1958540c-ab79-4694-b927-10918ce78fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb4052e22f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b0f6683-3249-4a28-8199-38cb9433ed07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 27]), torch.Size([27, 27]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets initialise some random weights\n",
    "W = torch.randn((num_classes, num_classes))\n",
    "xenc.shape, W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b8a2ee4-3b4b-4af2-a84c-fc53787a5798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (5, 27) @ (27, 27) -> (5, 27)\n",
    "(xenc @ W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3424d7b0-a93f-4087-afd0-e90ab389d049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.9482), tensor(-1.9482))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix multiplication is just efficient dot product in batches\n",
    "(xenc @ W)[3,13], (xenc[3] * W[:, 13]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5e1c814-708a-4d04-82d6-450fb41d7e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can't get probablities straight out of a NN, since they're supposed to sum to 1\n",
    "# can't get counts either, since they're only positive and are integers, NN doesn't output that\n",
    "# so instead we'll try to get interpret them as log(counts)\n",
    "# to get the counts, we exponentiate the log(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e767529-f8c0-44bf-816c-f73f304c4e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7288, -0.8358, -0.8506,  0.3732, -1.1215, -0.0884,  0.2791, -0.1276,\n",
       "          0.9450, -0.4415, -0.6371, -1.7877, -0.6919, -0.6392, -0.0134, -0.9629,\n",
       "          0.7049, -0.2726, -0.2014,  2.2627, -0.3677, -0.5461,  0.7511, -0.3949,\n",
       "         -0.5900,  1.1008, -0.1745],\n",
       "        [-0.5494,  2.4361, -0.7438,  1.0582,  0.5786, -0.4970, -1.2154, -1.5269,\n",
       "          0.2947,  1.3360,  0.1178,  0.1353, -0.3573,  0.0139,  2.1762, -0.2512,\n",
       "         -0.1329,  0.0729,  0.0994,  0.2169,  0.2958, -0.9179,  0.6608,  0.0218,\n",
       "         -0.5126, -0.0279,  0.7855],\n",
       "        [ 0.4982, -0.7476, -0.6848, -1.2135,  0.4826, -0.5418,  0.1298, -0.8656,\n",
       "         -2.4303,  0.8201,  0.2404,  1.9400,  1.0601, -1.9482,  0.0371,  0.2539,\n",
       "         -0.4406,  2.1144, -0.3692, -0.4772,  0.2471,  0.2653, -0.7836, -1.7515,\n",
       "         -0.8024,  0.0436,  0.5737],\n",
       "        [ 0.4982, -0.7476, -0.6848, -1.2135,  0.4826, -0.5418,  0.1298, -0.8656,\n",
       "         -2.4303,  0.8201,  0.2404,  1.9400,  1.0601, -1.9482,  0.0371,  0.2539,\n",
       "         -0.4406,  2.1144, -0.3692, -0.4772,  0.2471,  0.2653, -0.7836, -1.7515,\n",
       "         -0.8024,  0.0436,  0.5737],\n",
       "        [-0.3099,  0.5786, -0.1732,  0.7100, -0.3024,  0.4384, -1.0245, -0.8531,\n",
       "         -0.4852,  0.7450, -1.1435,  0.7740,  0.1818, -0.1456, -0.4061, -0.1422,\n",
       "          0.0319,  0.1173, -0.0790,  0.5251, -1.4985,  0.2355, -0.3745,  0.3323,\n",
       "         -0.5226, -1.8417, -0.9385]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41303417-4aaf-40b1-9185-a18b237125ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0726,  0.4335,  0.4271,  1.4523,  0.3258,  0.9154,  1.3220,  0.8802,\n",
       "          2.5729,  0.6431,  0.5288,  0.1673,  0.5006,  0.5277,  0.9867,  0.3818,\n",
       "          2.0236,  0.7614,  0.8176,  9.6086,  0.6923,  0.5792,  2.1193,  0.6737,\n",
       "          0.5543,  3.0067,  0.8399],\n",
       "        [ 0.5773, 11.4284,  0.4753,  2.8811,  1.7835,  0.6084,  0.2966,  0.2172,\n",
       "          1.3428,  3.8038,  1.1250,  1.1449,  0.6995,  1.0140,  8.8132,  0.7779,\n",
       "          0.8755,  1.0757,  1.1045,  1.2422,  1.3442,  0.3993,  1.9363,  1.0220,\n",
       "          0.5990,  0.9725,  2.1935],\n",
       "        [ 1.6457,  0.4735,  0.5042,  0.2972,  1.6203,  0.5817,  1.1386,  0.4208,\n",
       "          0.0880,  2.2708,  1.2717,  6.9588,  2.8867,  0.1425,  1.0378,  1.2891,\n",
       "          0.6437,  8.2850,  0.6913,  0.6205,  1.2803,  1.3038,  0.4568,  0.1735,\n",
       "          0.4483,  1.0446,  1.7748],\n",
       "        [ 1.6457,  0.4735,  0.5042,  0.2972,  1.6203,  0.5817,  1.1386,  0.4208,\n",
       "          0.0880,  2.2708,  1.2717,  6.9588,  2.8867,  0.1425,  1.0378,  1.2891,\n",
       "          0.6437,  8.2850,  0.6913,  0.6205,  1.2803,  1.3038,  0.4568,  0.1735,\n",
       "          0.4483,  1.0446,  1.7748],\n",
       "        [ 0.7335,  1.7835,  0.8410,  2.0340,  0.7391,  1.5502,  0.3590,  0.4261,\n",
       "          0.6156,  2.1065,  0.3187,  2.1684,  1.1994,  0.8645,  0.6662,  0.8675,\n",
       "          1.0324,  1.1244,  0.9240,  1.6906,  0.2235,  1.2656,  0.6876,  1.3941,\n",
       "          0.5930,  0.1585,  0.3912]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ W).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "831bfad5-734a-445a-9013-baa6168700e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = xenc @ W # interpret this as log counts, another word for these are logits\n",
    "counts = logits.exp() # we get the counts, if you think of the bigram model, the counts of each bigram\n",
    "probs = counts / counts.sum(1, keepdims=True) # once we have the counts, easy to get probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0b6a132-b819-48a8-ab39-511fcc408854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0579, 0.0121, 0.0119, 0.0406, 0.0091, 0.0256, 0.0369, 0.0246, 0.0718,\n",
       "         0.0180, 0.0148, 0.0047, 0.0140, 0.0147, 0.0276, 0.0107, 0.0565, 0.0213,\n",
       "         0.0228, 0.2683, 0.0193, 0.0162, 0.0592, 0.0188, 0.0155, 0.0840, 0.0235],\n",
       "        [0.0116, 0.2297, 0.0096, 0.0579, 0.0358, 0.0122, 0.0060, 0.0044, 0.0270,\n",
       "         0.0765, 0.0226, 0.0230, 0.0141, 0.0204, 0.1771, 0.0156, 0.0176, 0.0216,\n",
       "         0.0222, 0.0250, 0.0270, 0.0080, 0.0389, 0.0205, 0.0120, 0.0195, 0.0441],\n",
       "        [0.0418, 0.0120, 0.0128, 0.0076, 0.0412, 0.0148, 0.0289, 0.0107, 0.0022,\n",
       "         0.0577, 0.0323, 0.1768, 0.0734, 0.0036, 0.0264, 0.0328, 0.0164, 0.2105,\n",
       "         0.0176, 0.0158, 0.0325, 0.0331, 0.0116, 0.0044, 0.0114, 0.0265, 0.0451],\n",
       "        [0.0418, 0.0120, 0.0128, 0.0076, 0.0412, 0.0148, 0.0289, 0.0107, 0.0022,\n",
       "         0.0577, 0.0323, 0.1768, 0.0734, 0.0036, 0.0264, 0.0328, 0.0164, 0.2105,\n",
       "         0.0176, 0.0158, 0.0325, 0.0331, 0.0116, 0.0044, 0.0114, 0.0265, 0.0451],\n",
       "        [0.0274, 0.0667, 0.0314, 0.0760, 0.0276, 0.0579, 0.0134, 0.0159, 0.0230,\n",
       "         0.0787, 0.0119, 0.0810, 0.0448, 0.0323, 0.0249, 0.0324, 0.0386, 0.0420,\n",
       "         0.0345, 0.0632, 0.0084, 0.0473, 0.0257, 0.0521, 0.0222, 0.0059, 0.0146]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d70540a1-6293-4ecb-9b5d-85794e4f943a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c5a6599-8715-418f-b3bf-96903f3d06b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19,  1, 17, 17, 11])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(probs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b7b3f7-52ea-4fb7-9755-49c0bf363600",
   "metadata": {},
   "source": [
    "## Now let's do all of that without the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cff368fe-82c1-490b-b629-1e586224e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cff8b6d-c222-434d-9ce6-d5396c43401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dataset\n",
    "xs = torch.tensor(xs)\n",
    "# Labels\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17d9f1fb-9856-4924-a8b9-156335ad6540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((num_classes, num_classes), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cbaab7e-44ec-4170-999e-40c3bd341e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "xenc = F.one_hot(xs, num_classes=num_classes).float()\n",
    "logits = xenc @ W # interpret this as log counts, another word for these are logits\n",
    "counts = logits.exp() # we get the counts, if you think of the bigram model, the counts of each bigram\n",
    "probs = counts / counts.sum(1, keepdims=True) # once we have the counts, easy to get probabilities\n",
    "# the last two lines are also together called as 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79c12bcd-7083-42d1-8adf-0e0c9e60f19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5904fbd5-e146-4b4d-887a-0313b9700c55",
   "metadata": {},
   "source": [
    "## Let's now look into the quality of this untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2b8657e-cd79-4999-a329-bd3a2373823f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "bigram example 0: .e (indexes 0,5)\n",
      "input to neural net : 0\n",
      "output probablities from neural net : tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character): 5\n",
      "probablity assigned by the net to the correct character: 0.01228625513613224\n",
      "log likelihood: -4.399273872375488\n",
      "negative log likelihood: 4.399273872375488\n",
      "---------------\n",
      "bigram example 1: em (indexes 5,13)\n",
      "input to neural net : 5\n",
      "output probablities from neural net : tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character): 13\n",
      "probablity assigned by the net to the correct character: 0.018050700426101685\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "---------------\n",
      "bigram example 2: mm (indexes 13,13)\n",
      "input to neural net : 13\n",
      "output probablities from neural net : tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 13\n",
      "probablity assigned by the net to the correct character: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "---------------\n",
      "bigram example 3: ma (indexes 13,1)\n",
      "input to neural net : 13\n",
      "output probablities from neural net : tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 1\n",
      "probablity assigned by the net to the correct character: 0.07367686182260513\n",
      "log likelihood: -2.6080665588378906\n",
      "negative log likelihood: 2.6080665588378906\n",
      "---------------\n",
      "bigram example 4: a. (indexes 1,0)\n",
      "input to neural net : 1\n",
      "output probablities from neural net : tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 0\n",
      "probablity assigned by the net to the correct character: 0.014977526850998402\n",
      "log likelihood: -4.201204299926758\n",
      "negative log likelihood: 4.201204299926758\n",
      "===============\n",
      "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "num_inputs = len(xs)\n",
    "\n",
    "nlls = torch.zeros(num_inputs)\n",
    "for i in range(num_inputs):\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    print('---------------')\n",
    "    print(f'bigram example {i}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "    print(f'input to neural net : {x}')\n",
    "    print(f'output probablities from neural net : {probs[i]}')\n",
    "    print(f'label (actual next character): {y}')\n",
    "    p = probs[i, y]\n",
    "    print(f'probablity assigned by the net to the correct character: {p.item()}')\n",
    "    logp = torch.log(p)\n",
    "    print(f'log likelihood: {logp.item()}')\n",
    "    nll = -logp\n",
    "    print(f'negative log likelihood: {nll.item()}')\n",
    "    nlls[i] = nll\n",
    "    \n",
    "print('===============')\n",
    "print(f'average negative log likelihood, i.e. loss = {nlls.mean().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a187707d-1107-4e14-bedd-2ce50e2f037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function is differentiable\n",
    "# W is tweakable\n",
    "# gradient based optimisation to tweak W so as to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ed1b820-4ad8-412d-945b-4545f51a3f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4]), tensor([ 5, 13, 13,  1,  0]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(5), ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de4ddac0-9569-4f2f-b938-d31bd6c36e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0123),\n",
       " tensor(0.0181),\n",
       " tensor(0.0267),\n",
       " tensor(0.0737),\n",
       " tensor(0.0150))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0, 5], probs[1, 13], probs[2, 13], probs[3, 1], probs[4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fbbde0e-e223-4db2-9c25-41e2111e6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pluck out the probs at the correct indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "127d547b-2c91-4581-b87f-1ab3cf89fe64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0123, 0.0181, 0.0267, 0.0737, 0.0150])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[torch.arange(5), ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "669beeaa-6ca2-48e6-83cb-19354d4411fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((num_classes, num_classes), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fadf3ae-33ad-438d-8a30-220336be650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "xenc = F.one_hot(xs, num_classes=num_classes).float()\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(5), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4acf84e-d58f-4045-b088-e1c18c8428a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "W.grad = None # set to zero\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee73307f-055f-4428-b197-9e2589ff93da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 27]), torch.Size([27, 27]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape, W.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4298f46e-74b7-4c7a-80ac-04f4d9b4cbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7693049907684326"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85e2720e-e7b4-4ebf-8824-470c38c713e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update\n",
    "lr = 0.1\n",
    "W.data += -lr * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6dbf1c65-05e1-4bb4-ad91-2b79cae82438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "xenc = F.one_hot(xs, num_classes=num_classes).float()\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(5), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16d1fd30-6129-4172-ab07-7769f294242a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7492127418518066"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2f94bbe-ba22-48f5-b6c0-2dea53e09f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Observe that the loss is going down"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
