{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb536856-a2a4-4f7d-b5dd-22159cfa0807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop will not magically work optimally if you don't understand how it works under the hood\n",
    "# We will backprop by hand :muscle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81244a38-6a5e-4880-a1f9-3c7e95a60df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read through this\n",
    "# # https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b\n",
    "# When you clip the loss of an outlier you're setting its gradient to zero, and other subtle things like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a36dd6a0-4e05-4f83-8706-5660aef39cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Micrograd was only on individual scalars\n",
    "# It's not enough, let's think about autograd on the level of multi dim. tensors\n",
    "# Become better at debugging neural networks, making sure you understand what you're doing\n",
    "# EMERGE STRONGER\n",
    "# Nobody writes backward pass by hand, this is mostly for fun and exercise\n",
    "# Pretty cool bit of history trivia : RBMs, Matlab, No backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc36d4a-5ebe-4428-a5be-9204ec20491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd1ae8ac-bf26-4ca0-8802-774175d30b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ca7ec80-64ef-4d2d-917d-41e1aa50c58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32033,\n",
       " ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt').read().splitlines()\n",
    "len(words), words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b10dad4-e5cc-47f8-8299-b0973699a2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# let's only have one special token, and let's have it at index 0, offset others by 1\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "num_classes = len(stoi)\n",
    "vocab_size = len(itos)\n",
    "print(vocab_size, itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4675fef0-aeca-4019-943e-20d2bd82bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, block_size):\n",
    "    X, Y, = [], [] # X, input | Y, labels\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append moving window\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df489fc6-8b66-4cbb-8ba0-53aacaf34da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182625, 3]),\n",
       " torch.Size([182625]),\n",
       " torch.Size([22655, 3]),\n",
       " torch.Size([22655]),\n",
       " torch.Size([22866, 3]),\n",
       " torch.Size([22866]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splits\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "block_size = 3 # context length : How many characters do we take to predict the next one : 3 chars to predict the 4th\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1], block_size)\n",
    "Xva, Yva = build_dataset(words[n1:n2], block_size)\n",
    "Xte, Yte = build_dataset(words[n2:], block_size)\n",
    "\n",
    "Xtr.shape, Ytr.shape, Xva.shape, Yva.shape, Xte.shape, Yte.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ab4e8a-e7d9-407b-9993-dfe07c7d8f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare manual gradients to pytorch gradients\n",
    "# def cmp(s, dt, t):\n",
    "#     ex  = torch.all(dt == t.grad).item()\n",
    "#     app = torch.allclose(dt, t.grad)\n",
    "#     maxdiff = (dt - t.grad).abs().max().item()\n",
    "#     print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n",
    "    \n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13b205d4-587a-413d-b0d2-7b72fa67ca29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4137"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embed = 10   # dimensionality of characters in the embedding vector\n",
    "n_hidden = 64 # number of neurons in the hidden layer\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embed),             generator=g)\n",
    "W1 = torch.randn((block_size * n_embed, n_hidden), generator=g) * (5/3/(block_size*n_embed)**0.5)\n",
    "b1 = torch.randn(n_hidden,                         generator=g) * 0.1 # only for fun, is useless because of batchnorm\n",
    "\n",
    "W2 = torch.randn((n_hidden, vocab_size),           generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                       generator=g) * 0.1\n",
    "\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0   # non standard init\n",
    "bnbias = torch.zeros((1, n_hidden)) * 0.1         # non standard init\n",
    "\n",
    "# non standard init to be small numbers, sometimes if the tensors are init to zero, it can mask an incorrect implementation\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "sum(p.nelement() for p in parameters) # total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25a6e5e6-8da9-468a-b10b-b03dc21a1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "bs = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1bcf7ba-4b1b-4021-9103-99326a63373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get a batch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f514a8b-7885-4614-9780-8b48e70b3a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3353, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass - breaking down the forward pass into manageable chunks so we can inspect as needed\n",
    "\n",
    "# embedding \n",
    "emb            = C[Xb]                                        # embed the characters into vectors\n",
    "embcat         = emb.view(emb.shape[0], -1)             # concatenate the vectors\n",
    "# Linear layer\n",
    "hprebn         = embcat @ W1 + b1                       # hidden layer pre-activation\n",
    "# Batchnorm layer ------------------------------\n",
    "bnmeani        = 1/bs*hprebn.sum(0, keepdim=True)\n",
    "bndiff         = hprebn - bnmeani\n",
    "bndiff2        = bndiff**2\n",
    "bnvar          = 1/(bs-1)*(bndiff2).sum(0, keepdim=True) # Bessel's correction : divinding by n-1, not n\n",
    "bnvar_inv      = (bnvar + 1e-5)**-0.5\n",
    "bnraw          = bndiff * bnvar_inv\n",
    "hpreact        = bngain * bnraw + bnbias\n",
    "# Non linearity\n",
    "h              = torch.tanh(hpreact)                    # hidden layer\n",
    "# Linear layer 2\n",
    "logits         = h @ W2 + b2\n",
    "# cross entropy loss (same as F.cross_entry(logits, Yb)\n",
    "logit_maxes    = logits.max(1, keepdim=True).values\n",
    "norm_logits    = logits - logit_maxes                  # substract max for numerical stability\n",
    "counts         = norm_logits.exp()\n",
    "counts_sum     = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs          = counts * counts_sum_inv\n",
    "logprobs       = probs.log()\n",
    "loss           = -logprobs[range(bs), Yb].mean()\n",
    "\n",
    "# backward pass\n",
    "\n",
    "# zero out grads\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "# Retain the grad\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, norm_logits, logit_maxes, logits,\n",
    "          h, hpreact, \n",
    "          bnraw, bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani, embcat, emb]:\n",
    "  t.retain_grad()\n",
    "\n",
    "# backwara\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d10d395-cac5-4790-a5b2-d8936d2ab2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE will do a bunch of derivations for some equations such as cross entropy and batchnorm\n",
    "# and implement the efficient version by hand\n",
    "# We will also hand implement the rest of backward\n",
    "# In the end we will train using this hand implement functions and arrive at the same loss\n",
    "# This will be AWESOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "573d736d-9934-45a0-8b75-7f0f80ef4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start implement backward pass\n",
    "# 15 mins. in, this is going to be a great lecture, a bit intense even\n",
    "# think I have to come back and listen to it with a fresh head again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec082d6d-95ca-45b4-befe-6737e2e9fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when considering multiplications/additions with broadcasting rules applied \n",
    "# you need to sum(accumulate gradients) across the broadcasted dimension\n",
    "\n",
    "# the backward pass of a matrix multiply is a yet another matrix multiply with other element transposed\n",
    "# For a forward pass x = a @ b + c\n",
    "# dL/da = dL/dx @ aT\n",
    "# dL/db = aT @ dL/dx\n",
    "# dL/dc = dL/dx.sum(0)\n",
    "\n",
    "# More tricks on shape matching thinking\n",
    "\n",
    "# Tricky to take notes for this chapter, since it's mostly mathematical thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0982d9ec-591a-45a9-980b-39d95e651f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogprobs       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dprobs          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts_sum_inv | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts_sum     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dcounts         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dnorm_logits    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dlogit_maxes    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dh              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dW2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dlogits         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "db2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dhpreact        | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "dbngain         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "dbnraw          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "dbnbias         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dbnvar_inv      | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dbnvar          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "dbndiff2        | exact: False | approximate: True  | maxdiff: 2.1827872842550278e-11\n",
      "dbndiff         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "dhprebn         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "dbnmeani        | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "embcat          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
      "W1              | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n",
      "b1              | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "b1              | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "emb             | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
      "C               | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually\n",
    "# backward through all of the variables one by one\n",
    "# as defined in forward pass\n",
    "\n",
    "# Also the talk about Bessels correction\n",
    "# SCRUTINIZE YO' SHAPES\n",
    "\n",
    "# When we have a sum in forward pass, that turns to be broadcasting/replication in the backward pass\n",
    "# and conversely, when we have a broadcasting in backward pass, we have a sum in the forward pass\n",
    "\n",
    "# holds the derivate of loss wrt all the elements of logprobs, same size as logprobs\n",
    "dlogprobs                = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(bs), Yb] = -1.0/bs\n",
    "dprobs                   = (1.0 / probs) * dlogprobs # d(probs.log())/d(probs) = 1/probs\n",
    "dcounts_sum_inv          = (counts * dprobs).sum(dim=1, keepdim=True) # accumulate backward for the broadcasted term in forward pass\n",
    "dcounts                  = counts_sum_inv * dprobs\n",
    "dcounts_sum              = (-counts_sum**-2) * dcounts_sum_inv\n",
    "dcounts                 += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits             = counts * dcounts  # norm_logits.exp() already having been assigned to counts\n",
    "dlogits                  = dnorm_logits.clone()\n",
    "dlogit_maxes             = (-dnorm_logits).sum(1, keepdim=True)\n",
    "dlogits                 += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "dh                       = dlogits @ W2.T\n",
    "dW2                      = h.T @ dlogits\n",
    "db2                      = dlogits.sum(0)\n",
    "dhpreact                 = (1.0 - h**2.0) * dh                      # WHY NOT EXACT MATCH on my CPU ???\n",
    "dbngain                  = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw                   = bngain * dhpreact # shapes already correct, matches with forward pass\n",
    "dbnbias                  = dhpreact.sum(0, keepdim=True)\n",
    "dbndiff                  = bnvar_inv * dbnraw\n",
    "dbnvar_inv               = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar                   = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "dbndiff2                 = (1.0/(bs-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff                 += 2 * bndiff * dbndiff2\n",
    "dhprebn                  = dbndiff.clone()\n",
    "dbnmeani                 = (-dbndiff).sum(0)\n",
    "dhprebn                 += 1/bs * (torch.ones_like(hprebn) * dbnmeani)\n",
    "dembcat                  = dhprebn @ W1.T\n",
    "dW1                      = embcat.T @ dhprebn\n",
    "db1                      = dhprebn.sum(0)\n",
    "demb                     = dembcat.view(emb.shape)\n",
    "\n",
    "# Find where the gradient needs to flow back to in the embedding and accumulate them\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j] \n",
    "#dC.index_add_(0, Xb.view(-1),  demb.view(-1, 10))\n",
    "\n",
    "cmp('dlogprobs', dlogprobs, logprobs)\n",
    "cmp('dprobs', dprobs, probs)\n",
    "cmp('dcounts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('dcounts_sum', dcounts_sum, counts_sum)\n",
    "cmp('dcounts', dcounts, counts)\n",
    "cmp('dnorm_logits', dnorm_logits, norm_logits)\n",
    "cmp('dlogit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('dh', dh, h)\n",
    "cmp('dW2', dW2, W2)\n",
    "cmp('dlogits', dlogits, logits)\n",
    "cmp('db2', db2, b2)\n",
    "cmp('dhpreact', dhpreact, hpreact)\n",
    "cmp('dbngain', dbngain, bngain)\n",
    "cmp('dbnraw', dbnraw, bnraw)\n",
    "cmp('dbnbias', dbnbias, bnbias)\n",
    "cmp('dbnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('dbnvar', dbnvar, bnvar)\n",
    "cmp('dbndiff2', dbndiff2, bndiff2)\n",
    "cmp('dbndiff', dbndiff, bndiff)\n",
    "cmp('dhprebn', dhprebn, hprebn)\n",
    "cmp('dbnmeani', dbnmeani, bnmeani)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbbf73a3-2519-4b0c-9f96-6a48cd991406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too bad, the exact matches doesn't seem to work on this machine\n",
    "# I'm assuming it has to do with some floating point arithmetic but not sure\n",
    "# Even running Karpathy original nb gives me similar result on this machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "188dc157-276f-431d-a42c-832c858c9cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3353161811828613 diff : -2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2 : backprop through cross_entropy but all in one go\n",
    "# mathematical expression can be simplified analytically, backprop through less steps then\n",
    "\n",
    "# Previously forward pass cross entroy\n",
    "# cross entropy loss\n",
    "# logit_maxes    = logits.max(1, keepdim=True).values\n",
    "# norm_logits    = logits - logit_maxes                  # substract max for numerical stability\n",
    "# counts         = norm_logits.exp()\n",
    "# counts_sum     = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1\n",
    "# probs          = counts * counts_sum_inv\n",
    "# logprobs       = probs.log()\n",
    "# loss           = -logprobs[range(bs), Yb].mean()\n",
    "\n",
    "# now\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff :', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7bf3a44-b497-41bf-b681-6b2c8f679639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.402842700481415e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass with cross entropy loss\n",
    "# done on pen and paper\n",
    "\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(bs), Yb] -= 1\n",
    "dlogits /= bs\n",
    "\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3e2530c-54d5-4f3d-9f9c-075ea4cab962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dcaa885-caf0-4d39-a689-087337170acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index of the correct output for first row\n",
    "Yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "959db475-dc6f-4128-a60f-2e8916bac568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0687, 0.0888, 0.0184, 0.0504, 0.0196, 0.0814, 0.0246, 0.0364, 0.0190,\n",
       "         0.0307, 0.0353, 0.0359, 0.0371, 0.0294, 0.0336, 0.0133, 0.0089, 0.0189,\n",
       "         0.0153, 0.0552, 0.0497, 0.0228, 0.0258, 0.0729, 0.0600, 0.0273, 0.0205],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor(0.0190, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probs on the first row (first item in the batch)\n",
    "F.softmax(logits, 1)[0], F.softmax(logits, 1)[0][Yb[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d071ac6d-ebbb-49f2-98a5-c62ec98d1034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0687,  0.0888,  0.0184,  0.0504,  0.0196,  0.0814,  0.0246,  0.0364,\n",
       "         -0.9810,  0.0307,  0.0353,  0.0359,  0.0371,  0.0294,  0.0336,  0.0133,\n",
       "          0.0089,  0.0189,  0.0153,  0.0552,  0.0497,  0.0228,  0.0258,  0.0729,\n",
       "          0.0600,  0.0273,  0.0205], grad_fn=<MulBackward0>),\n",
       " tensor(-0.9810, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at the gradients (on the item, so denormalize by multiplying with batch size)\n",
    "dlogits[0] * bs, (dlogits[0] * bs)[Yb[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a7f0b0d-002c-4fb5-9573-345390ff54a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08b037ea-c7de-4509-8dbe-9125da451dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we want to do is\n",
    "# pull down the probability of incorrect characters and\n",
    "# pull up the probability of the correct characters\n",
    "#\n",
    "# cross entropy does this dynamically during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96063df9-c17d-44f7-b8e2-9b7f158fa446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff : tensor(2.6941e-05, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3 : backprop through batchnorm but all in one go\n",
    "\n",
    "# previously batch norm forward pass\n",
    "# bnmeani        = 1/bs*hprebn.sum(0, keepdim=True)\n",
    "# bndiff         = hprebn - bnmeani\n",
    "# bndiff2        = bndiff**2\n",
    "# bnvar          = 1/(bs-1)*(bndiff2).sum(0, keepdim=True) # Bessel's correction : divinding by n-1, not n\n",
    "# bnvar_inv      = (bnvar + 1e-5)**-0.5\n",
    "# bnraw          = bndiff * bnvar_inv\n",
    "# hpreact        = bngain * bnraw + bnbias\n",
    "\n",
    "# now\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True))\n",
    "print('max diff :', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5551fd13-c2c4-46c8-9074-d0824ac43a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "# done on pen and paper\n",
    "\n",
    "# calculate dhprebn given dhpreact (backprop through the batchnorm above)\n",
    "# not trivial at all tbh\n",
    "dhprebn = bngain*bnvar_inv/bs * (bs*dhpreact - dhpreact.sum(0) - bs/(bs-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16f7ee07-ff8f-478c-b58d-0e2dd2f2a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4\n",
    "# PUT THE MADNESS TOGETHER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae147248-7795-447a-b274-51ae72337f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da36ff72-5305-4525-ac74-a84044aa75c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4137"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embed = 10   # dimensionality of characters in the embedding vector\n",
    "n_hidden = 64 # number of neurons in the hidden layer\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embed),             generator=g)\n",
    "W1 = torch.randn((block_size * n_embed, n_hidden), generator=g) * (5/3/(block_size*n_embed)**0.5)\n",
    "b1 = torch.randn(n_hidden,                         generator=g) * 0.1 # only for fun, is useless because of batchnorm\n",
    "\n",
    "W2 = torch.randn((n_hidden, vocab_size),           generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                       generator=g) * 0.1\n",
    "\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0   # non standard init\n",
    "bnbias = torch.zeros((1, n_hidden)) * 0.1         # non standard init\n",
    "\n",
    "# non standard init to be small numbers, sometimes if the tensors are init to zero, it can mask an incorrect implementation\n",
    "\n",
    "bnmean_running = torch.zeros((1, n_hidden)) # the way we init W1 and b1, hpreact will be roughly unit gaussian, mean roughly zero\n",
    "bnstd_running = torch.ones((1, n_hidden))   # std dev roughly one\n",
    "\n",
    "\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "sum(p.nelement() for p in parameters) # total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8678ee00-c5ce-4a47-bb39-0733bf86c72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6011a112-1803-469e-9dd8-6e310230bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "bs = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "799cec60-f545-44ff-a5e0-b766a1e4683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "losslog10i = []\n",
    "stepsi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ab0ad99-15eb-483b-a649-ef974891352a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0/200000: 3.3285608291625977\n",
      "Loss 20000/200000: 2.0454561710357666\n",
      "Loss 40000/200000: 2.2798118591308594\n",
      "Loss 60000/200000: 2.241567850112915\n",
      "Loss 80000/200000: 2.248002290725708\n",
      "Loss 100000/200000: 2.2513630390167236\n",
      "Loss 120000/200000: 1.9825865030288696\n",
      "Loss 140000/200000: 2.05417537689209\n",
      "Loss 160000/200000: 2.132864475250244\n",
      "Loss 180000/200000: 1.9263790845870972\n"
     ]
    }
   ],
   "source": [
    "tot_steps = len(stepsi)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(max_steps):\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n",
    "\n",
    "        # forward pass\n",
    "        emb = C[Xb]                         # embed the characters into vectors\n",
    "        embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "        # Linear layer\n",
    "        hpreact = embcat @ W1 # + b1          # hidden layer pre-activation\n",
    "        # Batchnorm layer ------------------------------\n",
    "        bnmeani = hpreact.mean(dim=0, keepdim=True)  # bn mean for ith iter        <-----\n",
    "        bnstdi  = hpreact.std(dim=0, keepdim=True)   # bn std  for ith iter        <----- \n",
    "        hpreact = bngain * (hpreact - bnmeani)/bnstdi + bnbias     # normalize hpreact, also scale and shift\n",
    "        with torch.no_grad():\n",
    "            bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "            bnstd_running  = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "        # Batchnorm layer ------------------------------\n",
    "        # Non linearity\n",
    "        h = torch.tanh(hpreact)             # hidden layer\n",
    "        logits = h @ W2 + b2                # output layer\n",
    "        loss = F.cross_entropy(logits, Yb)  # loss function\n",
    "\n",
    "        # backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        # loss.backward()\n",
    "\n",
    "        # manual backprop\n",
    "        dlogits = F.softmax(logits, 1)\n",
    "        dlogits[range(bs), Yb] -= 1\n",
    "        dlogits /= bs\n",
    "        # 2nd layer backprop\n",
    "        dh = dlogits @ W2.T\n",
    "        dW2 = h.T @ dlogits\n",
    "        db2 = dlogits.sum(0)\n",
    "        # tanh\n",
    "        dhpreact = (1.0 - h**2) * dh\n",
    "        # batchnorm backprop\n",
    "        dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "        dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "        dhprebn = bngain*bnvar_inv/bs * (bs*dhpreact - dhpreact.sum(0) - bs/(bs-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "        # 1st layer\n",
    "        dembcat = dhprebn @ W1.T\n",
    "        dW1 = embcat.T @ dhprebn\n",
    "        db1 = dhprebn.sum(0)\n",
    "        # embedding\n",
    "        demb = dembcat.view(emb.shape)\n",
    "        dC = torch.zeros_like(C)\n",
    "        for k in range(Xb.shape[0]):\n",
    "          for j in range(Xb.shape[1]):\n",
    "            ix = Xb[k,j]\n",
    "            dC[ix] += demb[k,j]\n",
    "\n",
    "        # gradients\n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "\n",
    "        # update\n",
    "        lr = 0.1 if i < 100000 else 0.01    # switch learning rate\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            #p.data += -lr * p.grad\n",
    "            p.data += -lr * grad\n",
    "\n",
    "\n",
    "        # track stats\n",
    "        stepsi.append(tot_steps+i)\n",
    "        lossi.append(loss.item())\n",
    "        losslog10i.append(loss.log10().item())\n",
    "\n",
    "        # Print loss \n",
    "        if (i)%(max_steps/10) == 0:\n",
    "            print(f\"Loss {i}/{max_steps}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1620fe89-b8b8-4da7-beee-e8a992520dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be used for comparing - before fully using backprop grads\n",
    "# for p, g in zip(parameters, grads):\n",
    "#    cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3cc9294-5d49-4b89-a165-1d549911c4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 2.178114414215088\n",
      "val loss : 2.189039468765259\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking, can also the context manager instead\n",
    "def split_loss(split):\n",
    "    ds = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val':   (Xva, Yva),\n",
    "        'test':  (Xte, Yte),\n",
    "    }\n",
    "    x,y = ds[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1\n",
    "    hpreact = bngain * (hpreact - bnmean_running)/bnstd_running + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(f'{split} loss : {loss.item()}')\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f45c17-38f1-419d-83d2-f68f0444b548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "737b67e3-1687-45b2-9797-d4c49196178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note our losses going down with tricks\n",
    "\n",
    "# Train test validation split - in the beginning\n",
    "# Train Loss : 2.2589\n",
    "# Val Loss   : 2.2702\n",
    "\n",
    "# After increasing NN capacity and embedding size\n",
    "# Train Loss : 2.1190\n",
    "# Val Loss   : 2.1711\n",
    "\n",
    "# After fixing init weight bias softmax confidently wrong\n",
    "# Train loss : 2.0695\n",
    "# Val loss   : 2.1310\n",
    "\n",
    "# After fixing tanh saturation\n",
    "# train loss : 2.0355\n",
    "# val loss   : 2.1026\n",
    "\n",
    "# Now, with kaiming init and not using magic numbers for tanh saturation\n",
    "# train loss : 2.0376\n",
    "# val loss   : 2.1069\n",
    "\n",
    "# With batch norm\n",
    "# train loss : 2.0668\n",
    "# val loss   : 2.1048\n",
    "\n",
    "# With manual backprop\n",
    "# train loss : 2.176852226257324\n",
    "# val loss : 2.1903560161590576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e70f8e0-27a4-4a66-95b4-816ce0eae580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlah.\n",
      "amille.\n",
      "khi.\n",
      "mrix.\n",
      "taty.\n",
      "salanna.\n",
      "jazhnen.\n",
      "den.\n",
      "rha.\n",
      "kaqui.\n",
      "nelynna.\n",
      "chaiivon.\n",
      "leigh.\n",
      "ham.\n",
      "jock.\n",
      "quinn.\n",
      "salin.\n",
      "alianni.\n",
      "watelo.\n",
      "dearynix.\n",
      "kaeliigsa.\n",
      "med.\n",
      "edi.\n",
      "abetteley.\n",
      "rey.\n",
      "aras.\n",
      "ben.\n",
      "husya.\n",
      "samboson.\n",
      "kyloj.\n",
      "mikaaseren.\n",
      "kalla.\n",
      "lulo.\n",
      "zoe.\n",
      "sikora.\n",
      "krynn.\n",
      "amonleyno.\n",
      "adeneliah.\n",
      "bris.\n",
      "bria.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(40):\n",
    "    out = []\n",
    "    block_size = 3\n",
    "    context = [0] * block_size # initialize all with ...\n",
    "    while True:\n",
    "        # forward pass\n",
    "        emb = C[torch.tensor([context])] # (1, block_size, d)\n",
    "        embcat = emb.view(1, -1)\n",
    "        hpreact = embcat @ W1\n",
    "        hpreact = bngain * (hpreact - bnmean_running)/bnstd_running + bnbias\n",
    "        h = torch.tanh(hpreact)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1) # exponentiates the logits and then softmaxes them, similar to cross entroy no overflows\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # break if we reach the special token '.'\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbb9c857-0be3-4c4c-92ea-528e2aa79362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRAZY PART IS THAT\n",
    "# EVERYTHING IS THE SAME\n",
    "# NO Loss.backward - No Pytorch Autograd\n",
    "# Sort of similar - batchnorm is the most complex thing really"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45ee09-6fa4-4fb6-88ea-4c30dc564460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
