{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6912fe95-e748-434d-8b46-b74948486564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac6299d6-c7ae-4032-a0ee-7223bde0bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3922de0-a741-4f97-876d-b544642b206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of the initialization does not matter so much anymore because of modern innovations\n",
    "# One of such innovations is batch normalisation\n",
    "# Came out in 2015 from Google\n",
    "# Made it possibly to train deep neural networks quiet reliably\n",
    "# It made the training process JUST WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e765f4-b077-4988-8361-53dbc31b4896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32033,\n",
       " ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt').read().splitlines()\n",
    "len(words), words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "749ad72a-9e36-4f63-848f-0d6ddc49600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# let's only have one special token, and let's have it at index 0, offset others by 1\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "num_classes = len(stoi)\n",
    "vocab_size = len(itos)\n",
    "print(vocab_size, itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc7789bb-f552-4762-be51-aa6c34e612cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, block_size):\n",
    "    X, Y, = [], [] # X, input | Y, labels\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append moving window\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc34bfb8-2f98-403e-b035-45888b93a7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182625, 3]),\n",
       " torch.Size([182625]),\n",
       " torch.Size([22655, 3]),\n",
       " torch.Size([22655]),\n",
       " torch.Size([22866, 3]),\n",
       " torch.Size([22866]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splits\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "block_size = 3 # context length : How many characters do we take to predict the next one : 3 chars to predict the 4th\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1], block_size)\n",
    "Xva, Yva = build_dataset(words[n1:n2], block_size)\n",
    "Xte, Yte = build_dataset(words[n2:], block_size)\n",
    "\n",
    "Xtr.shape, Ytr.shape, Xva.shape, Yva.shape, Xte.shape, Yte.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a0bfd5-a560-45c3-9892-dc414a1a4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the training loop below we have the hidden state h pre activations\n",
    "# we don't want hpreact to be way to small -> then the tanh doesn't do anything\n",
    "#                    or to be way to big   -> then the tanh is saturated and no gradients flow back\n",
    "# we want them to be rougly gaussian\n",
    "# the insight from batch norm. paper is\n",
    "# you have the hidden states that you want to be gaussian\n",
    "# so WHY NOT TAKE THE HIDDEN STATES and NORMALISE THEM TO BE GAUSSIAN\n",
    "# YOU CAN JUST DO THAT because normalising the hidden states is perfectly differentiable operation\n",
    "# So let's just do that\n",
    "# In the following training loop we are trying to roughly make hpreact gaussian\n",
    "# Since we want to normalise in the batch\n",
    "# we are going to calculate the mean and std. dev across the batch and then use that to normalize the batch\n",
    "# while calculating the mean we also want to keep the dimension so we can broadcast it\n",
    "# the normalisation ensures that every single neuron and it's firing rate will be unit gaussian across this batch\n",
    "# hence call batch normalisation, since we are normalising the pre non linearities across the whole batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ad6e7ec-686d-4d03-ab28-1c7878a1d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the problem with this is that\n",
    "# we want these to be roughly gaussian but only at init\n",
    "# we don't want them to forced to be gaussian always\n",
    "# we want the neural network to let the distribution move around a bit depending on how it needs to be optimised\n",
    "# we want the backprop to optimize how to move it around\n",
    "\n",
    "# so in addition to the idea of this normalisation, we also need the idea of scale and shift as mentioned in the paper\n",
    "# we take the normalised input, additionally scale them by some gain and and offset by some bias to get final output from the layers\n",
    "# this means we need to add these learnable parameters to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e41bcd-751e-4199-b037-2a6807b933ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12297"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embed = 10   # dimensionality of characters in the embedding vector\n",
    "n_hidden = 200 # number of neurons in the hidden layer\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embed),             generator=g)\n",
    "W1 = torch.randn((block_size * n_embed, n_hidden), generator=g) * (5/3/(block_size*n_embed)**0.5)\n",
    "b1 = torch.randn(n_hidden,                         generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),           generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                       generator=g) * 0\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))  # we want the initial values to affect the hpreact in a nice gaussian\n",
    "bnbias = torch.zeros((1, n_hidden)) # we want the initial values to affect the hpreact in a nice gaussian\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "sum(p.nelement() for p in parameters) # total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f19813ba-1b9c-408e-87e2-61d193d1963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a43ee295-62ba-477e-b1c8-8cfb0dac2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "losslog10i = []\n",
    "stepsi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7cfa13e-35af-49dc-a8e4-571c4fc3ef2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0/200000: 3.3146889209747314\n",
      "Loss 20000/200000: 2.3374552726745605\n",
      "Loss 40000/200000: 2.011928081512451\n",
      "Loss 60000/200000: 2.4775009155273438\n",
      "Loss 80000/200000: 2.278811454772949\n",
      "Loss 100000/200000: 1.9473825693130493\n",
      "Loss 120000/200000: 1.9836552143096924\n",
      "Loss 140000/200000: 2.3839313983917236\n",
      "Loss 160000/200000: 1.9733015298843384\n",
      "Loss 180000/200000: 1.9972705841064453\n"
     ]
    }
   ],
   "source": [
    "tot_steps = len(stepsi)\n",
    "\n",
    "for i in range(max_steps):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]                         # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    hpreact = embcat @ W1 + b1          # hidden layer pre-activation\n",
    "    hpreact_mean = hpreact.mean(dim=0, keepdim=True)  # hpreact mean\n",
    "    hpreact_std  = hpreact.std(dim=0, keepdim=True)   # hpreact std dev\n",
    "    hpreact = bngain * (hpreact - hpreact_mean)/hpreact_std + bnbias     # normalize hpreact, also scale and shift\n",
    "    \n",
    "    h = torch.tanh(hpreact)             # hidden layer\n",
    "    logits = h @ W2 + b2                # output layer\n",
    "    loss = F.cross_entropy(logits, Yb)  # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01    # switch learning rate\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    stepsi.append(tot_steps+i)\n",
    "    lossi.append(loss.item())\n",
    "    losslog10i.append(loss.log10().item())\n",
    "    \n",
    "    # Print loss \n",
    "    if (i)%(max_steps/10) == 0:\n",
    "        print(f\"Loss {i}/{max_steps}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7493128a-7431-4228-ae98-4471a8cf7df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2201be6f50>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABeVklEQVR4nO3deViU5foH8O8wwAAqiyKbouBKLoBCEuZWkmBWmvXLzFI5pWXZcsg0WzCXE2Ye81imHUvNNq1T2aKRiaKpiIr7miKIG6AYq8r6/v5AxhmY7Z39Hb6f65pLZ+ZdnpeBmXue537uRyYIggAiIiIiCXGydQOIiIiIxGIAQ0RERJLDAIaIiIgkhwEMERERSQ4DGCIiIpIcBjBEREQkOQxgiIiISHIYwBAREZHkONu6AeZQV1eHS5cuoVWrVpDJZLZuDhERERlAEASUlZUhKCgITk7i+lQcIoC5dOkSgoODbd0MIiIiMsL58+fRvn17Ufs4RADTqlUrAPU/AE9PTxu3hoiIiAxRWlqK4OBg5ee4GA4RwDQMG3l6ejKAISIikhhj0j+YxEtERESSwwCGiIiIJIcBDBEREUkOAxgiIiKSHAYwREREJDkMYIiIiEhyGMAQERGR5DCAISIiIslhAENERESSwwCGiIiIJIcBDBEREUkOAxgiIiKSHAYweuw6cxXf7jtv62YQERGRCodYjdqSnvg0EwBwR4Anerf3snFriIiICGAPjMEuFl+3dROIiIjoFgYwREREJDkMYIiIiEhyGMAQERGR5DCAISIiIslhAKNDdW2d8v83qmtt2BIiIiJSxQBGhzpBUP6/ulbQsSURERFZEwMYIiIikhwGMIZiBwwREZHdYACjgwwyWzeBiIiINGAAYyCBXTBERER2gwGMDjKVDhiB8QsREZHdYAAj0pWySkxesw9bTxXauilERETNFgMYAzV0wLzzyzFsOl6AxFV7bdoeIiKi5owBjA6aUngLSm5avR1ERCROXZ2AvwrKUFfH8X9HxQCGiIgczsJNpzDsg+3418YTtm4KWQgDGAMxiZdIv5Lr1dh9tggC/2DIxj5OzwYAfLYjx8YtIUthAKODTNZ0EIlvy8bhB1rzkPCf7Xj8v7ux/uBFWzeFiBwcAxgbyL1agVobj8ueKSzDuxtPoKi80uLnyjp3DdHzNmP9AX6oObrLt3LEUo/m27glROTojApgli5dipCQELi5uSEmJgZ79uzRuu3q1ashk8nUbm5ubmrbCIKA5ORkBAYGwt3dHXFxcTh9+rQxTbMYcxWy+1/WBQxZmI6pX+83y/GMdd8H2/Hf7Wcx4/sjFj/X5DVZKKqowivrDlr8XM1J6tHLSP7pKGpUVk0nImouRAcw69atQ1JSEmbNmoX9+/cjIiIC8fHxKCzUXhfF09MTly9fVt7OnTun9vyCBQuwZMkSLF++HJmZmWjRogXi4+Nx86ZtZ/yoDiCdLig36hiCIKDkerXy/vJt9eOyv9n4G2rDiM6Ri8UWP1cth48s4rkv92NNxjn8sJ89W0TU/IgOYBYtWoRJkyYhMTERPXr0wPLly+Hh4YGVK1dq3UcmkyEgIEB58/f3Vz4nCAIWL16Mt956CyNHjkR4eDjWrFmDS5cuYf369UZdlCWsycgFID6X452fjyFiziZsPl5ggVYRAYVlnNpPRM2PqACmqqoKWVlZiIuLu30AJyfExcUhIyND637l5eXo2LEjgoODMXLkSBw7dkz5XE5ODvLz89WO6eXlhZiYGK3HrKysRGlpqdrNEtSWEjDyGJ9n1Pc2Lfj9pOkNIiIiSdt6shDPf5WFvyuqbN0UyRMVwFy9ehW1tbVqPSgA4O/vj/x8zUMi3bt3x8qVK/HTTz/hyy+/RF1dHfr3748LFy4AgHI/McdMSUmBl5eX8hYcHCzmMsxm66lCjP54J7KvGDe8REBlTa2tm0BEZDWJq/di45F8fqk1A4vPQoqNjcX48eMRGRmJwYMH44cffkDbtm3xySefGH3MmTNnoqSkRHk7f/68GVtsuMRVe7E/rxhTvz5gk/NL3R/HC9D9rVST6jTM+eU4EhZvx40qBkLWcuRCCWJT0vATp0oTGa2g1PIzQB2dqADG19cXcrkcBQXq+RwFBQUICAgw6BguLi7o06cPzpw5AwDK/cQcU6FQwNPTU+1mCap1YHSlvpRcN29XYHlljVmPZ69eWVsf+M399bjRx1i5Mwcn88tsWnekorIGF4tv2Oz81jblqyxcLrmJl9ce1LqNTONCHERE5iMqgHF1dUVUVBTS0tKUj9XV1SEtLQ2xsbEGHaO2thZHjhxBYGAgACA0NBQBAQFqxywtLUVmZqbBx7SW0pvVFi9k987Px9Br1u/Y/tcVC5/JsdTZcKZTv39txt3ztyCv6LrN2mBN1Raetl18vQrfZ11ARTMJ5InIOKKHkJKSkrBixQp8/vnnOHHiBKZMmYKKigokJiYCAMaPH4+ZM2cqt58zZw42bdqEs2fPYv/+/XjyySdx7tw5PPPMMwDqezleeeUVzJs3Dz///DOOHDmC8ePHIygoCKNGjTLPVUrI6l25AID3fz9l24aQwSpuDV/tPltk45Y4hqc/34dXvzuEmT9YvkYREUmXs9gdxowZgytXriA5ORn5+fmIjIxEamqqMgk3Ly8PTk6346K///4bkyZNQn5+Pnx8fBAVFYVdu3ahR48eym2mT5+OiooKTJ48GcXFxRgwYABSU1ObFLyzNVN6RdilTmSYrHN/AwB+OXwJS8b2sXFriMheiQ5gAGDq1KmYOnWqxufS09PV7n/wwQf44IMPdB5PJpNhzpw5mDNnjjHNsZqpXx9Anw7eJh2DawIRERGZjmshiVRVw7LtRIY6dL4Yr39/GFetsOYWETUvDGBEOnbJMkXziCyhurYOpwvKLNrz9+mfZ/HRFvW1yxom8I1cuhNr957Hmz8an8+SV3Qd11j0y2Z2nrmKpz7LbDZJ6tbCpALTMYAxg+vV9leD5KYdtoksQ1ds8szn+3DfB9vxv6wLFjl3VU0d5m04gYWb/kJh6e0lDWSN3p3PFBpX7LGw9CYGvb8Vfef+gYrKGryXehJHLpSY0mQSadynmfjz9FW8so71rsi+MIAxg9pa/d9uG7+hA8CqnboLuH3651ksSRO/KnfyT0cR9nYqjttJb1FzT/spu1mN178/jJ1nrlr93NtuJZ43zG4zN9Xp65UWGF5V7fH896a/sCw9Gw9+tMPs5zHF2SvlysRjR1ZYxmFAsi8MYMxIEAT8d3s2dmUb9kE1+5fj+KugTONztXUC5m04gUV//IXLJepF0q5X1eCZz/fi232aKxCvubX+0odbxAc/zUVtnYCa2jqrFKD7z+bTWLv3PMZ9mmnxczmyk/n2EZA3du+/t+GRZbtw/hqHWHQpuVGN6f87hIxslhsg82AAY0abjhfg3Y0n8cSKTGSd+xvv/HxM7z5F5ZrH9lU7LW5Wq3+zXbUzF5tPFGL6/w4DAHacrh+jNuYN1Ja9I9Y4dU1tHY5dKlHLAflkWzZ6JKei9zubcPf8Ldhx2rI9I+f/bh4fbDts0MNkT7gmmm7vpZ7Et/suYOyK3bZuCjkIBjBmdK6oQvn/R5btsli3fcmNarX7T35WP0b96PJdWL4t2yLnlKrX/ncYI5bswMfpt38uKb+dRGVNHW7cyhN6/YfDtmqeQ2HhOdKFScBkbgxgzKBMRMnzkhuWK49eUFqJ+b9pX+E0r+h6k+DH0f14oH6NpI+3ntG6zYW/byBl4wnl/XNFFVi06RRnvpBDOXS+GMcuMQGaHAcDGDPZ9tcVnLysOZ9Fla3qYeQVXceg97cics4mo/a/UVWLZenZOFOo/xpr6wSkbDyBP44X6N3WXnyy/azy/w99tBNLtpzBtO8OiTuIA8+L5FRaaSu5UY2RS3dixJIdqKtz/Kz6wxeKbd0EvTRN7CBxGMCYyYSVe/DDAeNXRC65UY1fDl3S+Jyxs4lU81syc4qaPAbczkP54I+/8OiyXVqnX3+w+S+8l3oScYu26z3vL4cu4ZPtZzFpzT5jmm2UTcfMFyw19FJtOVmI9FOFWLUzB89+sc/iixiK8f7v2nvazOnC39dRVVMneiotl86wL0UqX5xsufCptXy1O8/WTSArYABjJ55evRcvfqP5w+GFr/erfXheMuPMmSu3pkb+J+009p37Gz8d1ByEHcwrNviY+Sr1QKxlm4VW7564ai9m/3Icvx8rwE8HNQeYqpakncagBVuVP1dLWbrV8rlOWeeuYcB7WzFq6U7lY/Y8lbayphYvfXMA31uo5o0+jh8WENkXBjBWUnxdc+6JcOttb5+eOhKqAcyvhy+br2GNVBlQ0+ZmdS1W78xB7tUKvds6kutV+vOXFv3xF/KuXcfH6dpzbuxd8fUq7M/7G//Lqg9mj1+2z+nLja3dcx4/H7qEV8UO/RGRJDGAsZL80ptYoZJnYS62GNZYuvUM3vnlOIYsTNe7bU1tndUSh3dlX0Xxdc2JtxVVlq9MvExlppMpeQZfZZ7D3fO3aJyWe7O61uI/z4ELtmL0x7tMWn3dbESMRP2t5bW3GjvqgtmTcw3zfzuJyhr7qMh95EJJs59mT+Zn1GrUZJx/qcx00cfQ9+2+c/7Q+pxg4Dvqr4f1D42oysy5ZvC2vx3Nb/JY2c1qtHJz0bnf+WvX0baVAm4ucoPP9cSK+kJxufNHGLyPOeVo6JHKvVqBv7X0vmnz5o9Hb/17BGsnx6o9FzlnU5O6QOZWdrO+p8kaRf50YRaN8R77JAMA0MrNGS/c08XGrYHdVU8mx8AeGIkTM4Vbm6lfW26NkxuNej5+2H8Bvd/ZpNZb0djRiyUYuGAr7kpJw6Zj+ZJd1+li8Q0MWZiOPSICPlW1GnpxNAUvhoSpzSBvkzRoCKr58pMprlfV4LMdOXZXbZoBjB2osdAwkD1+aDVMTX4v9aTW9v12tD7Hp/h6NSZ/kYV3RfRc6ZNfYr0EY02Jz19k5BqUSyN57D6xWzI7n78rCAI+Tj9j0hCmob3P1iAIgpb3ePt+HVTN/+0k5v56HPGL9c9CtSYGMHZAV29Ec1TdKJFY25pPxhi4YIvZjmWMt386hnkbDAvI9ub+bVCi9I3qWosFwcaSyluzmLpMdXUCynX0eBaW3cSYTzJED8laW9nNauzJudYkT+tqeSV+OnjR5nkzW04WYkHqKYxfucfoY9yw8DCrGC98vR995/6BEpFDyfZk1631q65bIZdQDAYwtiYA3+9vOu3T3DM/jOmNqaiswY8HLtT/4RnwiVRVU6dWb8IYb/x4BP81U7Kzpkq6jYMjW9h2yvBvloYkSi9Lz0bcom3K+5lni5C07qDaa3Gm0P7W6dl15irW7TW8XscuMy8C+N/t2Yiet9ng5Tcmrt6LXrN+1xpUzv31BDJzrll0SNYcRi3dicc+ycD/VN53qmrqED1vM15eexCLNv2lfFywQTfuhb9Ny7368cAFrTW1bGHjkXyU3qzBhiOWmz3aXDGAsTFrvT2oBkmakk0bqMYpb/x4BP9cd8jggnTxi7cjat5mk8ZJv87U/YFWVF6Jf204rrPK77d7z0MQBPSdqz3B2RiaYjhTAzZzyVWpkDvmv7vxw4GLmKWymGiVHQ5TPvFpJmZ8fwRHLtimvP27G+uLAepafkNVw5DGd1maewR19c6oEgQB/1i9F+NX7tEYIBy9WKLzb9RUDb8rqh/y//7jlPL/vx+7nXi/X0T9J00Ky26i9KbmnoeKyhqctcACmP9cx2n0zQVnIdmBXAuVZ1d9azxwvlj5/3sM+FYPQFm4bU/uNdwR6Kl3+4Y33bQThSptMG+IFjVvMwBgxZ85+PH5/hq3mf79YfxfdHuznlebKV/uN8txUjaewM3qWswe2cssxwMsWy+owYdbTpt8jIvFN9C7vZcZWiMNpTdrsOVk/d/IlbJK+Hm6KZ+7UlaJBz6sn7Fjzdl03+3TXPzP0KBMk5Ib1ej3rzSNz32x+xyWpJ3GlbJK/Dz1boS39zbLOal5YQ+MjVlyTHHzifpeito6AVU1pn0DP6EypKVvVo3qt/0dZ8zb7a9q91nt7dBUMdbUn8GV8qZDUntyNbdBTNh2s7oWn2w/i88zzlklyfjoxRJ8t6++l6q2TsB/t2fjkEqAK8bHZsjf+kVLzkidAKw3YXkOu6Xjl+P83xb6MmODkdPTBdrXTXt7/VFlterNjXpT3//9lKZdiJpgAGNjlpwi3PCm9cq6g4bvY4bzquaeNF72QFt9N2PKv/+l4w1y9Me7mjzW7a3fRJ9D1ZI003sbgKaLuKl+uFijMOEDH+7Aa/87jKVbz+A/m//CuxtPYqTKcgH6mPvDcMPhy1qTaQ1ZvsGWLL1kBFC/7tXoj3ca/F6heQjX9rlf2lwqudksFpg01ModOXhl7QGNZRRsxV6T8hnASIQpHxpiEtoqzNx9m6VniYQGr353CJlnxfXW/Kjj27mlirAdu3Q7X8NcCY76uswvl9zQmkegSmx7Fm76C0u2iF/ywJj1kLKv6M7puP8/f4o+ZmO2mCGRZ2JdjBsGBCVLt2Zjf16xQX/H+SU3MXDBVpPaZO1p1v/LuiB+5XcHoO3HPOfX41h/8JLOPD+qxwDGxj4y4gPEkub/dtLowmtiNX7zPqslcdHSlWfFWL7tLIrKK1FZU6uW7KiJocGgvumisSlbEP7OJr3H+X6/6cMt5vrWJ/ZD0ByLRJpr9pqlfaMy80p19lhjKRtPIP6D23U3DJlBd/hCsUltayxLyxCpuf1g4aFCW8ymakxsXNgs6kWZiAGMjZ3SMQxiDrqGWbRpKENOTV38+zqi5m3GvQu34fw17b08e3KuYfr3hzU+1zgX54SZpsxvNMM0TX1BgCmfA5/+eRYvfL0fL3ytOfHZ3DNS6uoE7Dpz1aDeK2P9rWGqvj6qs550BSWfbD9r8vtDnY4XzJDX8hOJBIVA/dCZtiG9YR+IK8D286FL+PO0HawFRjoxgHFwYv9wG9hj3RBLF2v74I+/9PZANEwr1TdEdTJf+wdPYVklss7Vf7PVVKvGlsxZNLCxeRtOYMPhy9hw+LLG677339p7I4zxxe5zeOLTTI35UObyjIElBmxl68lC/Rs1Yq/5DrqU3KjGwAVbcee/Nmt8/rSI97Pz167jpW8O4KnP9uCVtQesWr1bm+tVNdj21xWTJyIYy16LN3MatUTUCQJe++4QWiis85Lp6tpWda2iCq1buFq4NfXBxUdbLTvc9p+00wjydtO/oRk8siwD/xfVHt8ZkbyszZaThUb1CNiCti79MjP2ljTkSNljMG6M3WeLsDf3Gt5+oIfBf3MNeUGaftz2VG5fm6qaOrg66/+enWfGUhSFZbcDlvUHL+FKeSW+euYuk497qfgGXtfSK6vPi18fQNrJQkzsH4J3HuppclscBXtgJOK17w7ju6wLWL0r19ZNUWOtcdr/pJ22Slb+icuWHdJTZc7gpUEfMxfvs5TzWqqtrttruR4ge1ZyvRp1dYLO3o+fD13CjwcuYt6vx63WLlvTNQRmLblXNQdHjReq1efDLWewVuX3W9c088bSbvWkfbn7nNZtBEFAXtF15ZeD5J+O4rkvsiAIAs4VVeC91JNWmTVnTQxgJCJVT8KoIzB3AqIx7C1AVHW5xDIzq2xhlJZp2+killlwFEcvliBiziaDh6M01Yo5mV+Kx5ZnYK85km5VoijV8MEWwwjppwptvjaTJn+evoI7klPx703G16xpXMD0SlmlSUX85v92EoPe34qlt3qq12ScQ+qxfJwqKMPoj3dhWXo2Xl7bdJmLspvVmP/bSRy7VKK1Z1RmpwOLDGDIbmw6xmmDusSm2HYhSjHKb1bjNSOmxu44c9Wg7VSnmC5JO43hZpiGbStfZNR/q95iRL5Kg4kr92JP7jWs+DNH7XGh0b+GUOvptHEHyHNf7sc7P5u/x6movBJVNXWYtGafUT1aDct0fGimWaTF16tw5782o9es343av65OUCZcL1RZywoAamoFFN0aWt6XW1/WIudqBVbtzMHN6lq8u/Eklm/LxoglOxDzbppd5PwYijkwZLJ9udfwhY6uTTJctgXWhjHUmcIys63Bs9XCPSmT1uxTltpf9MdferZ2fKp5G5qIWVbiXNF1HLlQgt7tvQzKkyksvYnHV+zGE/064JmBnQw+j6G+2ZOHlNG9NT5XUHoTX2ScE7UUxbmiCgx+P13tsbce6GFKE012/JJpMxF11cTSpGE5mb8rqtQWDi4sq8SHW07jXw+r/7ztNYnXqB6YpUuXIiQkBG5uboiJicGePYYte7527VrIZDKMGjVK7fGJEydCJpOp3RISEoxpGtnAo8szzFIxtUgiCaiWNHHVXpud+7FPdtvs3JZgzHh/eWUNRn60Q9kNb4+qaurw1GeZ+OTWKtqlN6u1VrhusHZP00VSdaWXzL3VK9F4mremirnv/34KZ69UYN6GE3pabn7PfL4PH209g2e/yDJ4n9+OihuOv1h8A6OW7hSVs2Jtxha925trWKFReyU6gFm3bh2SkpIwa9Ys7N+/HxEREYiPj0dhoe7uz9zcXEybNg0DBw7U+HxCQgIuX76svH3zzTdim0Y28Lkd54yQuo/Tz+icim5vU7pNUV5Zo3Oq+89aqtp+kXEOhy6U2PV6PIculODP01eRcquezDOr9efOiC0UuCf3WpNegZLr1ej37mbM+J/6TBrVZPQP004bva6WMY5cNHwl85k/HEHiqj16qydrCuwOni/G81/tx+LNf+Ghj3aITuA1hpj8ZVMrQkuV6ABm0aJFmDRpEhITE9GjRw8sX74cHh4eWLlypdZ9amtrMW7cOMyePRudOmnuYlQoFAgICFDefHx8xDaNbKDxmHtzYg8zJMRYkHrK5DH7K1rWLLI3mtcDuu2lb5omMwJokjBqT+vRaFJyvVrrgqKmun+Jel5R6c0aXC2vwjodtYL+/cdfynW17G3Y4Zs9edh66gq+zmzaG2WI4hvVWLz5NA5fKMFlG+WJ1Gj5fTyuoximrkTjjLNFKLthuUKPliYqgKmqqkJWVhbi4uJuH8DJCXFxccjI0F69dc6cOfDz88PTTz+tdZv09HT4+fmhe/fumDJlCoqKtK+LU1lZidLSUrUbkbVJMXj7j4kLUtqqkJY1/HvTKSzefPvnU3KjGtHz/tA4c8NerMnI1fm8TMc2loy/Z/5wGN/tE18mIP2U8YnMUiUIgsFrxgH1CwCfzC81eHkEfflojZdwse+QXZ2oAObq1auora2Fv7+/2uP+/v7Iz9c8rrhjxw589tlnWLFihdbjJiQkYM2aNUhLS8N7772Hbdu2Yfjw4ait1dxNl5KSAi8vL+UtODhYzGUQmYW2FZRJmhr3Tv188CL+vl5tlvyuj7acVltCQJVqT4XYNaRq9XyIVdbUIfmnY6KOaagzheVah5C/2XNereaJIW5W19o0B0wffTlVOVcrjCoFse2vK/i3iET0Jz/NRMLiP+1+pXZrsOgspLKyMjz11FNYsWIFfH19tW73+OOPK//fu3dvhIeHo3PnzkhPT8fQoUObbD9z5kwkJSUp75eWljKIISKjlWjoRi+9qbkmx/lrNyAIgsHBRk1tnXJq61OxHY1vpJ0xtFq3oSol3rvXMLMnyMsNo/u2x7T47nr3OXqxRGvto1e/O4Q+HbzRqW1Ltcf33eqt+WZPHkb1aWdao7X46eBFfLTlDJY/FYXOjc5vT0T1wPj6+kIul6OgQD3juaCgAAEBAU22z87ORm5uLh588EE4OzvD2dkZa9aswc8//wxnZ2dkZ2drPE+nTp3g6+uLM2c0j9crFAp4enqq3YiIjFWqIYDRlsj786FLmP2L4bVDVPtINA3BVakkVqdstN5MHiksJSAF+SU3cUklYfxSyU18tPUM/jhegNm/6O79euDDHTqfv/ff23AgT/Pw0o3qWuzKvmqRNeJeXnsQpwvLMf1/xi19YC2iemBcXV0RFRWFtLQ05VTouro6pKWlYerUqU22DwsLw5EjR9Qee+utt1BWVob//Oc/WntNLly4gKKiIgQGBoppHhE1I78evoRFmzR3vetaE8ocq3av3pVrtjVpfth/u4ZHZo5lEnLJcu5KSdP4+CQzLfS5LF3zF/3DF0rwxIpMJN3XzSzn0cQas61MIXoIKSkpCRMmTEB0dDT69euHxYsXo6KiAomJiQCA8ePHo127dkhJSYGbmxt69eqltr+3tzcAKB8vLy/H7Nmz8cgjjyAgIADZ2dmYPn06unTpgvj4eBMvj4gc1dSvtSfXfrZDe4L181/tF30ucy4yaS92n72Gqpo65FytED2F/mS+BSZO6OgQ2nqyEPeE+Zn/nLfsOH0VA7r66muGRZi6fImmGj9aGTDqqWmWltjcLGsRHcCMGTMGV65cQXJyMvLz8xEZGYnU1FRlYm9eXh6cnAwfmZLL5Th8+DA+//xzFBcXIygoCMOGDcPcuXOhUCjENo+IqImqmjqU3KhG21bGvaf8JbKImbYcmf0iZptYQ7e3fjNqv+VaegUsJXH1XmXlZUt48rNMZL4xFP6ebtjWDNfj0sc+wxcjk3inTp2qccgIqJ8Orcvq1avV7ru7u+P3341b/4GIyBD3L/kTZwrLsTlpsFXOd++/t2FgV1/MGaneA/2qEetD6VKuJdHY0tZbYAbMjWrbDld8v/8CRvdpj4/srArzJiOr7DYHXMyRiBxO486PM4X1a0ylHjU9/6WBrjocOVcrsCbD8uuDfapjqExqtOWSNHax+AYKSs1fSG5B6imDFxO1J5dEFNWrqqlDqYjhUDsdOVLiYo5EREZYvu2srZvQrFyvqsFr/zuMDbcWpjz77v1mP8c0M/eQ2YquQH31zlzRx6uosk1Pnz7sgSGiZkPbjNOBC7aKPtaC3zUXpiPLWJaerQxeAO1l9Ql47kvtierVRky7Pldkn2stMYAhombjg82GVzxV9b3KVGcxBEHAL1oWjiRxLhXbZv0hsl8cQiIih1NRad6EUGMXAPw4PduuV7Ym0uXYpVLs11JIzx6wB4aIHE7GWe2LwVoTgxeyN2IX8Rz98S7LNMQMGMAQERnBkqs5k35/cHqxUbafdpw6NwxgiIjI7jWe0vvC1+IrKlP9EgSOggEMERERSQ4DGCIisnt1nDZNjTCAISIiu7fBDKuIk2NhAENERHavskZ8ATZybAxgiIiISHIYwBAREZHkMIAhIiIiyWEAQ0RERJLDAIaIiIgkhwEMERERSQ4DGCIiIpIcBjBEREQkOQxgiIiISHIYwBAREZHkMIAhIiIiyWEAQ0RERJLDAIaIiIgkhwEMERERSQ4DGCIiIpIcBjBEREQkOQxgiIiISHIYwBAREZHkGBXALF26FCEhIXBzc0NMTAz27Nlj0H5r166FTCbDqFGj1B4XBAHJyckIDAyEu7s74uLicPr0aWOaRkRERM2A6ABm3bp1SEpKwqxZs7B//35EREQgPj4ehYWFOvfLzc3FtGnTMHDgwCbPLViwAEuWLMHy5cuRmZmJFi1aID4+Hjdv3hTbPCIiImoGRAcwixYtwqRJk5CYmIgePXpg+fLl8PDwwMqVK7XuU1tbi3HjxmH27Nno1KmT2nOCIGDx4sV46623MHLkSISHh2PNmjW4dOkS1q9fL/qCiIiIyPGJCmCqqqqQlZWFuLi42wdwckJcXBwyMjK07jdnzhz4+fnh6aefbvJcTk4O8vPz1Y7p5eWFmJgYrcesrKxEaWmp2o2IiIiaD1EBzNWrV1FbWwt/f3+1x/39/ZGfn69xnx07duCzzz7DihUrND7fsJ+YY6akpMDLy0t5Cw4OFnMZREREJHEWnYVUVlaGp556CitWrICvr6/Zjjtz5kyUlJQob+fPnzfbsYmIiMj+OYvZ2NfXF3K5HAUFBWqPFxQUICAgoMn22dnZyM3NxYMPPqh8rK6urv7Ezs44deqUcr+CggIEBgaqHTMyMlJjOxQKBRQKhZimExERkQMR1QPj6uqKqKgopKWlKR+rq6tDWloaYmNjm2wfFhaGI0eO4ODBg8rbQw89hHvuuQcHDx5EcHAwQkNDERAQoHbM0tJSZGZmajwmERERkageGABISkrChAkTEB0djX79+mHx4sWoqKhAYmIiAGD8+PFo164dUlJS4Obmhl69eqnt7+3tDQBqj7/yyiuYN28eunbtitDQULz99tsICgpqUi+GiIiICDAigBkzZgyuXLmC5ORk5OfnIzIyEqmpqcok3Ly8PDg5iUutmT59OioqKjB58mQUFxdjwIABSE1NhZubm9jmERERUTMgEwRBsHUjTFVaWgovLy+UlJTA09PTrMcOeX2DWY9HREQkVbnzR5j1eKZ8fnMtJCIiIpIcBjBEREQkOQxgiIiISHIYwBAREZHkMIAhIiIiyWEAQ0RERJLDAIaIiIgkhwEMERERSQ4DGCIiIpIcBjBEREQkOQxgiIiISHIYwBAREZHkMIAhIiIiyWEAQ0RERJLDAIaIiIgkhwEMERERSQ4DGCIiIpIcBjBEREQkOQxgiIiISHIYwBAREZHkMIAhIiIiyWEAQ0RERJLDAIaIiIgkhwEMERERSQ4DGCIiIpIcBjBEREQkOQxgiIiISHIYwBAREZHkMIAhIiIiyWEAQ0RERJLDAIaIiIgkx6gAZunSpQgJCYGbmxtiYmKwZ88erdv+8MMPiI6Ohre3N1q0aIHIyEh88cUXattMnDgRMplM7ZaQkGBM04iIiKgZcBa7w7p165CUlITly5cjJiYGixcvRnx8PE6dOgU/P78m27du3RpvvvkmwsLC4Orqil9//RWJiYnw8/NDfHy8cruEhASsWrVKeV+hUBh5SUREROToRPfALFq0CJMmTUJiYiJ69OiB5cuXw8PDAytXrtS4/ZAhQ/Dwww/jjjvuQOfOnfHyyy8jPDwcO3bsUNtOoVAgICBAefPx8THuioiIiMjhiQpgqqqqkJWVhbi4uNsHcHJCXFwcMjIy9O4vCALS0tJw6tQpDBo0SO259PR0+Pn5oXv37pgyZQqKioq0HqeyshKlpaVqNyIiImo+RA0hXb16FbW1tfD391d73N/fHydPntS6X0lJCdq1a4fKykrI5XJ8/PHHuO+++5TPJyQkYPTo0QgNDUV2djbeeOMNDB8+HBkZGZDL5U2Ol5KSgtmzZ4tpOhERETkQ0TkwxmjVqhUOHjyI8vJypKWlISkpCZ06dcKQIUMAAI8//rhy2969eyM8PBydO3dGeno6hg4d2uR4M2fORFJSkvJ+aWkpgoODLX4dREREZB9EBTC+vr6Qy+UoKChQe7ygoAABAQFa93NyckKXLl0AAJGRkThx4gRSUlKUAUxjnTp1gq+vL86cOaMxgFEoFEzyJSIiasZE5cC4uroiKioKaWlpysfq6uqQlpaG2NhYg49TV1eHyspKrc9fuHABRUVFCAwMFNM8IiIiaiZEDyElJSVhwoQJiI6ORr9+/bB48WJUVFQgMTERADB+/Hi0a9cOKSkpAOrzVaKjo9G5c2dUVlZi48aN+OKLL7Bs2TIAQHl5OWbPno1HHnkEAQEByM7OxvTp09GlSxe1adZEREREDUQHMGPGjMGVK1eQnJyM/Px8REZGIjU1VZnYm5eXByen2x07FRUVeP7553HhwgW4u7sjLCwMX375JcaMGQMAkMvlOHz4MD7//HMUFxcjKCgIw4YNw9y5czlMRERERBrJBEEQbN0IU5WWlsLLywslJSXw9PQ067FDXt9g1uMRERFJVe78EWY9nimf31wLSY9u/i1t3QQiIiJqhAGMHhHtvW3dBCIiImqEAQwRERFJDgMYIiIikhwGMERERCQ5DGD0kMls3QIiIiJqjAEMERERSQ4DGCIiIpIcBjB6SL/MHxERkeNhAENERESSwwCGiIiIJIcBjB4cQSIiIrI/DGCIiIhIchjAEBERkeQwgCEiIiLJYQBDREREksMARo+4O/xs3QQiIiJqhAGMHp5uLrZuAhERETXCAIaIiIgkhwGMHqwDQ0REZH8YwBAREZHkMIDRQ2brBhAREVETDGCIiIhIchjAEBERkeQwgCEiIiLJYQBDREREksMAhoiIiCSHAQwRERFJDgMYPVjIjoiIyP4wgCEiIiLJMSqAWbp0KUJCQuDm5oaYmBjs2bNH67Y//PADoqOj4e3tjRYtWiAyMhJffPGF2jaCICA5ORmBgYFwd3dHXFwcTp8+bUzTiIiIqBkQHcCsW7cOSUlJmDVrFvbv34+IiAjEx8ejsLBQ4/atW7fGm2++iYyMDBw+fBiJiYlITEzE77//rtxmwYIFWLJkCZYvX47MzEy0aNEC8fHxuHnzpvFXRkRERA5LJgiCqDSPmJgY3Hnnnfjoo48AAHV1dQgODsaLL76I119/3aBj9O3bFyNGjMDcuXMhCAKCgoLw6quvYtq0aQCAkpIS+Pv7Y/Xq1Xj88cf1Hq+0tBReXl4oKSmBp6enmMvRa+eZqxj3aaZZj0lERCRFufNHmPV4pnx+i+qBqaqqQlZWFuLi4m4fwMkJcXFxyMjI0Lu/IAhIS0vDqVOnMGjQIABATk4O8vPz1Y7p5eWFmJgYrcesrKxEaWmp2o2IiIiaD1EBzNWrV1FbWwt/f3+1x/39/ZGfn691v5KSErRs2RKurq4YMWIEPvzwQ9x3330AoNxPzDFTUlLg5eWlvAUHB4u5DCIiIpI4q8xCatWqFQ4ePIi9e/fiX//6F5KSkpCenm708WbOnImSkhLl7fz58+ZrLBEREdk9ZzEb+/r6Qi6Xo6CgQO3xgoICBAQEaN3PyckJXbp0AQBERkbixIkTSElJwZAhQ5T7FRQUIDAwUO2YkZGRGo+nUCigUCjENN1o7X3crXIeIiIiMpyoHhhXV1dERUUhLS1N+VhdXR3S0tIQGxtr8HHq6upQWVkJAAgNDUVAQIDaMUtLS5GZmSnqmJbSsU0LWzeBiIiIGhHVAwMASUlJmDBhAqKjo9GvXz8sXrwYFRUVSExMBACMHz8e7dq1Q0pKCoD6fJXo6Gh07twZlZWV2LhxI7744gssW7YMACCTyfDKK69g3rx56Nq1K0JDQ/H2228jKCgIo0aNMt+VEhERkcMQHcCMGTMGV65cQXJyMvLz8xEZGYnU1FRlEm5eXh6cnG537FRUVOD555/HhQsX4O7ujrCwMHz55ZcYM2aMcpvp06ejoqICkydPRnFxMQYMGIDU1FS4ubmZ4RKJiIjI0YiuA2OPLFkHBgBCXt9g9mMSERFJjWTrwBARERHZAwYwREREJDkMYIiIiEhyGMAQERGR5DCAISIiIslhAENERESSwwCGiIiIJIcBDBEREUkOAxgiIiKSHAYwREREJDkMYIiIiEhyGMAQERGR5DCAISIiIslhACNS5htDbd0EIiKiZo8BjEjeHi62bgIREVGzxwCGiIiIJIcBDBEREUkOAxgiIiKSHAYwREREJDkMYIiIiEhyGMAQERGR5DCAISIiIslhAENERESSwwBGJBlktm4CERFRs8cAxgCq1XddnfkjIyIisjV+GhMREZHkMIAxgCDYugVERESkigEMERERSQ4DGCIiIpIcBjBEREQkOQxgiIiISHKMCmCWLl2KkJAQuLm5ISYmBnv27NG67YoVKzBw4ED4+PjAx8cHcXFxTbafOHEiZDKZ2i0hIcGYphEREVEzIDqAWbduHZKSkjBr1izs378fERERiI+PR2Fhocbt09PTMXbsWGzduhUZGRkIDg7GsGHDcPHiRbXtEhIScPnyZeXtm2++Me6KiIiIyOGJDmAWLVqESZMmITExET169MDy5cvh4eGBlStXatz+q6++wvPPP4/IyEiEhYXh008/RV1dHdLS0tS2UygUCAgIUN58fHyMuyIr8PdU2LoJREREzZqoAKaqqgpZWVmIi4u7fQAnJ8TFxSEjI8OgY1y/fh3V1dVo3bq12uPp6enw8/ND9+7dMWXKFBQVFWk9RmVlJUpLS9Vu1vRafJhVz0dERETqRAUwV69eRW1tLfz9/dUe9/f3R35+vkHHmDFjBoKCgtSCoISEBKxZswZpaWl47733sG3bNgwfPhy1tbUaj5GSkgIvLy/lLTg4WMxlmOzRqPbYPXOoVc9JREREtzlb82Tz58/H2rVrkZ6eDjc3N+Xjjz/+uPL/vXv3Rnh4ODp37oz09HQMHdo0UJg5cyaSkpKU90tLSy0axMTd4Y/v919ASBsP5WMBXm469iAiIiJLEhXA+Pr6Qi6Xo6CgQO3xgoICBAQE6Nx34cKFmD9/PjZv3ozw8HCd23bq1Am+vr44c+aMxgBGoVBAobBeHsrskT3Rp4M3hvX0178xERERWZyoISRXV1dERUWpJeA2JOTGxsZq3W/BggWYO3cuUlNTER0drfc8Fy5cQFFREQIDA8U0z2JaKpzx5F0d4deKvS5ERET2QPQspKSkJKxYsQKff/45Tpw4gSlTpqCiogKJiYkAgPHjx2PmzJnK7d977z28/fbbWLlyJUJCQpCfn4/8/HyUl5cDAMrLy/Haa69h9+7dyM3NRVpaGkaOHIkuXbogPj7eTJdJREREjkR0DsyYMWNw5coVJCcnIz8/H5GRkUhNTVUm9ubl5cHJ6XZctGzZMlRVVeHRRx9VO86sWbPwzjvvQC6X4/Dhw/j8889RXFyMoKAgDBs2DHPnzrXqMBERERFJh0wQBMHWjTBVaWkpvLy8UFJSAk9PT6udN+T1DVY7FxERka3lzh9h1uOZ8vnNtZBM8I+7QxET2hpBnJFERERkVVadRu1okh/sAQAYuGCLjVtCRETUvLAHhoiIiCSHAYyNTYjtCHcXua2bQUREJCkMYGxs9sheODqb08WJiIjEYABjBu29PfRvpIPcSWamlhARETUPDGDM4N+PRSC+pz/WTb7L1k0hIiJqFjgLyQyCvN3xyVP6l0hQlfnGUHi4MveFiIjIGAxgbMTfk7VjiIiIjMUhJCIiIpIcBjBEREQkOQxgiIiISHIYwBAREZHkMICxQzMSwvDh2D62bgYREZHdYgBjJ5aN6wsfDxd89UwMpgzpjAcjgmzdJCIiIrvFadR2YnjvQCT0CoBMxqq8RERE+rAHxo4weCEiIjIMAxgiIiKSHAYwFtTCVY4Fj4Y3efz7Kf1t0BoiIiLHwQDGQh6/MxgHZw3DY9HBTZ4L9W1hgxZpxzWZiIhIahjAWIhMBrjINf94W7dwFX28ADOunfRI3/Zq95l5Q0REUsMARiIyZt5rtmMt/L9wpE8borz/zkM9AQAv3NPZbOcgIiKyJE6jlghzzlCSyWRQuNyOXYf1DEBCrwC0cnPB0q3ZZjsPERGRpbAHRuLMlU/Tys3FrMcjIiKyJAYwZjawqy8AYFxMR+VjR2fHY/7o3rZqkii/vTzQ1k2wmNYtXBHe3svWzSAiIjPgEJKZfZ7YDyU3quGjkqjbUuEM35YKG7bKcG4ujjsjac8bQ1FeWYPIOX/YuilERGQiBjBm5uQkUwteGgg2aIsugpENcpHLUF0rYObwMHi6uyCqow+GfbDdvI2zEGe5E2Scc0VE5BAYwEicIR/Hf/xzEO7TEWSIyQ9Of+0e7MkpwgPhQVqniRMREVkaP4EkbONLA9GhjYfe7dzNWKiunbc7Hu7TnsGLmbVU8LsEEZEY/BSSqJ5BnugR5MkBET0mDQy1dROI7Jq3h4utm0BkFKMCmKVLlyIkJARubm6IiYnBnj17tG67YsUKDBw4ED4+PvDx8UFcXFyT7QVBQHJyMgIDA+Hu7o64uDicPn3amKbZLQYa5uEj8s32oYh2eCy6Pf79fxEAzNsbZSv/eTzS1k0gBzGwqy8Gdm1r62YQGUV0ALNu3TokJSVh1qxZ2L9/PyIiIhAfH4/CwkKN26enp2Ps2LHYunUrMjIyEBwcjGHDhuHixYvKbRYsWIAlS5Zg+fLlyMzMRIsWLRAfH4+bN28af2XNhKYCd55uTYcj/nG39HoiNicNMvkYcicZFjwagUei6pdPcHV2kvxU8aiOPrZuAjkITzf2vpB0iQ5gFi1ahEmTJiExMRE9evTA8uXL4eHhgZUrV2rc/quvvsLzzz+PyMhIhIWF4dNPP0VdXR3S0tIA1Pe+LF68GG+99RZGjhyJ8PBwrFmzBpcuXcL69etNujipmzuyp9mO1d7HXfQ+fq3MN/U7J+V+0ft08Wul83k3F+NGQDu1tUyxvmOz4y1y3MbMuS6WvdD0mjjiddoj9g6TVIn6BKiqqkJWVhbi4uJuH8DJCXFxccjIyDDoGNevX0d1dTVat24NAMjJyUF+fr7aMb28vBATE6P1mJWVlSgtLVW7OaLokNZmOY5MJoNPC9t+05LJZAhplHDs10qBBY+EizrO8N6BAIBu/i3xQHiQ3u2DW4sP3Ixlb1PlpS4y2NvWTSAiOyZq6sPVq1dRW1sLf39/tcf9/f1x8uRJg44xY8YMBAUFKQOW/Px85TEaH7PhucZSUlIwe/ZsMU2XpHY6ek101XH5533dMPuX42qPPRgehF1nihDTqQ2A+m+3Ee294Cx3QisrzYB575FwjPnvbgD1vScfju2Dc0XXRR3j7RE9ENXBB0O6t4XcSQYXuQzuLs5YuTNH4/atrNRFPi6mAwRji+uQRi8N7YrUY5rfA8hMZOLKKBDZE6vO3Zw/fz7Wrl2L9PR0uLkZ3z08c+ZMJCUlKe+XlpYiODjYHE20K55uLtj22hC4OjftKFvwqPaei4n9Q5oEMM5yJ7x/K5EVqC+4t/6FuwGYd6FIXVTPc2x2AuROMlRU1Ri8fys3Z7i7ypX5LACQMjocG49c1hrAOCJDwqSEngHIvlKOId3bYsWf0vzZcHYMEekiagjJ19cXcrkcBQUFao8XFBQgICBA574LFy7E/PnzsWnTJoSH3/7wbdhPzDEVCgU8PT3Vbo6qY5sWCPRS74m5I9ATvdppX9PH0IBEJpNZLXgBgNYqw1hyp/rz3tPdz+D9LdnSzibmxYy4NbRlL+aO6oU/kgZLdmmIvh28bd0EIkl558Eetm6C1YkKYFxdXREVFaVMwAWgTMiNjY3Vut+CBQswd+5cpKamIjo6Wu250NBQBAQEqB2ztLQUmZmZOo/ZnDnZqMv3zfvvMGn/Ln6t8NaIO7BkbB/lY9YMoHT5aeoAjY8HeunvKfxwbB/07+Ir6nzPDAjFI33b69/QSK01LGchFf07t8F3z/W3dTOaDfv4CyRTTZTgTFNTiR5CSkpKwoQJExAdHY1+/fph8eLFqKioQGJiIgBg/PjxaNeuHVJSUgAA7733HpKTk/H1118jJCREmdfSsmVLtGzZEjKZDK+88grmzZuHrl27IjQ0FG+//TaCgoIwatQo811pM+ZipojHyQzHeWZgJzO0xHqiOvrg18OXdW7TPUD3bClbkIt4rfw9FSgorRR17No6y+X7eLq5iGo/ETVPouehjhkzBgsXLkRycjIiIyNx8OBBpKamKpNw8/LycPny7Tf8ZcuWoaqqCo8++igCAwOVt4ULFyq3mT59Ol588UVMnjwZd955J8rLy5GammpSnkxzpamXxE8i01HFFqmzN1JN4U1+oOl0/Y46lqj4RUtvFTkWa5UFIDKWUUm8U6dOxdSpUzU+l56ernY/NzdX7/FkMhnmzJmDOXPmGNMcSbircxu4yp0QFmi5b+sZM+9tki8jJe4ucowe0B6f7ZBm0qkpLNXfcKfIqfjvPxqO2joBxTeqMf83zTMLLZFcGxbQChHtvfHjgYt4dnB9L52djC7alZfu7YIlW85Y5Vwt7HB9LjcXJ9ysrrN1M8hOcC0kK2mpcMaR2cOw/vm7TT6Wttm6tgpewsw4hOLvab7iedqIme1sqaGMzn4t4Weha33yrg7K/w/q1rRM/PIno7T2dnVq2xKP9+sAuY7oofGPT1ctH0OSmyf2D8G8Ub2x6LEIHHlnGPp0YKVhbbr6299wpTV1t+H192Fiud1hAGNFCme5WfJIVNnDl1RzrqQ8PjYEj0W3x/Ino5o816al5YIbGZomRwe3dsf0hLAm2xpbAXj/2/fh62diMCMhDI9FB+OFe7oYdRx95o7spXZf0WgafkKvAPibcVgxPFj7jDhDvPNQT7Ru4QqZTGa1uj10W//O4hLQmysPM66j1r9zG7MdqzljAENaGdOF/9zgziad081FjgWPRiChV9Mp9J881TSoAcwXxO19Mw6hvrenU/85/V6081bv1Xrx3i7YnDTYqPO3buGK/l18MWVIZ8idZGYN/Bo83Kddk5ldndu21LmP2Ne5cbvbNgosR0bqr5BM9uPRqPYY3bedUfs+Fm25mXSOatfr9+LrSXeZ9ZiLx0Sa9XhSwQCGzMqYNZcM0cJVjm4W7j5u01KB8Pa6exNeHdYd7X20J7ja2sCuTb9Nawv8xHrz/jvw+yuDmvRANe4Z66syBDT1Xsv0MpmTpqB740sDjf5QF0PsUheWSBR3cpLhgXDj6hix+LQ4LnIZgrzN+x658P8iMKqP5X9X7REDGDJYbx3F8xo0dI1aMwHTnO+hbs5Nu4n/GdcNAPDUXR11t6NRQ07MSRB1bk2riBtyHn2CW98OuGI7Gd913dW/pegp4138WuLIO8OMHnYTw9hZbJqC1lDfFhgX00HD1ub169SmK6Ovm6z927k9LFcxd1Qv/RtpMG1YNzO3hAD7SCOwFQYwZJCIYG/0C9U/o6VT25bYOm0IDiYPs0KrzG9YT/8mj700tAv++OcgzH5I3Org7iLHzO8IvF1RWnUoSwxtgeOCR8MxNMwPn02M1rxBI57u4oa3VPOHVD9kZahfj+rQLMv/Ppgzl8PdVQ4nK0ThXhqCLl1roFlKgKfh5+xk5O/mgxFNhxbNOQHAXKRWgqi3nl5jR8YARoJsMb10aJj2kv+NvxOG+raAl7s0kzE1zTqSyWTo6t/K7AnYupgzYRAAHosOxmcT74SHq2GBycN92pt11oVCQ8+WufUIcowlRWxRnbpHkKdVV263lTHR+tfM0/bznzzItPw+S7H00Lo9YwBDBnGWi39TbXgf4KJ84jXUb7HVt0FXZyf8+PzdmNg/xOzHjtfQy6WJTGTnuFTXfbIXf06/19ZNMEmLW0G/rqE/Y4PyPW8OxWANJQnIthjAkE5J93VDzyBPvfkfmqx//m4M6OKLr58xLOPe9qP79mN6Qne8cX8Ytrw6xKrn7dBae4KyrqnXYl67J2LE/y7ZgiHXZMhaWfqMjzX852Eva4fZo0+eisb3U2J1zoQ05MenaRO/VtKoZm5O/3rYuFwna2IAQzq9NLQrNrw00Kj6HBHB3vjymRiH6dq3Jg9XZ0we1BkhRuYbiLX9tXuw8aWBaNtKe60d1RwdqrfmH/1wb5gffnrB+AKVc0b2wiEJ5YzpigHSXh3c5DEPV3mT0gO6GDLMo4nCxQlRHVtbdB2tPW8ONWn/R6OMuzZL2/V6fe/bC/d0xrxRvbDpn4MwTgJfNBjAkMPq1c66H7iqs4j8bgUCDbN+GteTsTcd2ngYHGjawUQYk71x/+0ChaZ83HX1b4WVE+9ERLC3Se0xZojWlhpmfMX1UB8O1FRz6OE+7dDFT3ctIlXvPaq9srOtmdoTM+vBHmZph4erHL4tFViVeKdZjhfk7Y7c+SPwWnwYnryro2TyahjAkMksObXzpVt1RIyZuunspPnX21XuZJFEaNXu/Yb/fvhEH7x0bxese9a8hau0tsGIj2NpfXRq18676YfL8if7wl1Dboxcy++GNh890Ud0e36ealyvjD1MldZJBmydNgT/ey4Ww3oYls8kJfqS9TX9PhlKU57WgkfD4SoX9/s4qGtb7H1zKO7prn1yRXPAAEaCVN/fHH1IPGlYdxxKHobRfc1X8dPJSYbjs9VrtJjypqSLb0sFkgwsfnfvrZle9pT0bG/TXFNG99Zay8ZdwwyrhF6BBk8db0z1T+uBcP3VhScP6tRof8P/OO317/jP6fdofNzbwxXRIa0NyskRc21J9xlfK8bY0gONJfQM0Lkau7Eev9O8w0fMhzJyNWqyfy5yGaprTfsmZ8y3eUvQVCvDVKo1Wu4N87PqFGlt/jEgFO183NFP5ArSlvR/0cEor6wxqAaQNYzt1wGhvi2Q8d8ig/dppbBOQPjG/Xfgv9vPWuVc1hLc2gPtvN1xsfiGVc4nZvX0wd3aYttfV5T3fW9VhDb1c93fU4Ftr92DkUt34tD5YtMOpuIdkXWkSD/2wDiob5+NRWSwN76fEmvrptg9ZxODF3N8Eerk1xIucic8EB4EPz0LLVoz1pI7yfDMwE4Ib+9tvZMSWZC+GktT7+2q83nByPmSnOZvfgxgHFSfDj5Y/8LdiOpoH9+cLUn18zzAxFWWteXNWMqvLw7AqMggLBWRY+Escrzc2lTf3k3p5lbddWy/YGx4aYDxjZKYR6PED5n++Hx//PriAJ1LERjjPjPlubT38UDntoYN8zTMeDP3qs0yyLB9+j06y0JItQhnc2Tf74QkCTMS6md0PD0g1MYtAXa/IX6ao+rU4djObdC/cxuLFHDTpFc7Lyx+vI9JC0R+MCbCjC2yTzMSwtAzyLiS6cZ+IJmaSmvsN3XAuIJrHq7O6NXOCzGd2hiVdKzN68PDsMAMM4PkTjL88U/DplP/+Hx/bPrnIIODJ02LmGrj21Jh9sDIVC0U7J0xBgMYMllMpzY4Picebz9gnimC1vLp+Ggk9AzAa/HdlY/JnWT4etJdkhqvfriP+RKcHVGHNh5IttLvprWn7qsKVJmFJTdxXFO1oKGbixyPGVmbpbHGuWYxWnKr3Fw0rz6vLSS8y4RFSrdOG2LTHC8ZgNWJ/dDFryVWTTTPtGhD+LZ0tdq5LIUBDJmFoWvs2JO4Hv5Y/lQUvD2k/4fcwB4mJkSaWBPFWLpq7UzQ0aNmD5OWVafR6vp91PXyehpRbNISlo3ra/C2M4aHqd3v29Fb7b6hv85BGqbQGyrUtwXC29l2QcSIYG9sThqMe3SsOaeq8c/JGFL7wqkJAxgiCzBnLs08I2rg2JIxVZtN1bGNB7r4tcSycX3xj7ttP5QplrPcCb+9PBC/vjgALRXO6K3hA/WhiCC00fKt+ZtJ1qkzZIjgRstRaJvN2KG1h9oXn8ei24te9HNV4p14fkhnPBTRTnxDJSzRDL/jCmfpf/xL/wpI8mY/1BPOTjJ8MCbS1k0x2X8ej0SglxuWPWn4t1B9nryro841ikw1QET+gL1qGDIZ3jsQ0SE+ere3RrE4sWUI7gj0RK9bgYumJR2WjO2jtZcp1oo5HZYqr+BkRPfhPd39MD0hTPTyAb1s3ONiKhc7T+S3Fun1+5PDmdA/BONiOtj97BpDjIxsh5GR0vo2aIsekwbaciB0EVOWvoHdV7dtRFtzO7ZpgYcigvDzoUvWbZAd0vaKtm2pgIerHNeraps+eSvOCW7tgd9fGWRU0Uhb/ir1bufVpGCiJS1/MgrPfZlltfOJJf1PjGbOUWoLOELwYkkfjImAm4sT5oyUTnKxpfi2VGDrtCHYY8SMM2tr76PeY2KO4UBzTWt2VM5yJxxIvk9vNd3uAa10rrBuLWJKDfzy4gA8GKG/KjRgnhyXhF4BaveN+cJhSfzUkCDV7tI37r8D3fxbSmLpc0eyY4bmEuuWEtWxNY7NTsD42BCd2xmayCmFDgnflgq083ZHex/3JtcV6ttCa8E/e7q276f0V7v/pI76I4ayh0RtazG2jpDCWS56fSFH0s7bHfd0b6tnK8N+tqp1heztd6/5vsISNH90b/h7KvD+/92uyRDk7Y5N/xxs1qXPG6aCjuxjWKRvb/w96/MH4u6w3EJnqrkI7bwtl5+iStc4f8ro3njqro4Y3E3fm5ZlfPl0DDzdnA2qPxLodTvw6K5jrSW5kwzbXhuC9GlD9C/1YMIbq7ZdzREI+Xu6mVxcUR9jaha5OjvB1dkJ/4wzfu0hc7OnwFPqzDlkGmPCFHVLYw6MhDzerwPG3Bls8UW8vnr6LmScLVIuLig1v744EHtyrmFYT8t1tctkMhydHY/aWkFtXSVbGduvg03Pf3cXXxyaNQwymQyVNbVwktXn1qjGHV9PikFhaSW6+bfCz1Pvxq+HL+PFW6uNayO1oUUfK0/JPzUvQePMHddGM0z6dPDGgbxi5f3p8d0xsX+I5H6+ZLynB4Tisx05tm6GWfG3V2KssQKpl4cLEnoFNHkTlIq2rRQYER5o8Uz9lgpniyw0KVUNv5sKZzmOz0nA3jfj1H5f+3f2xag+9QnO4e298cb9d1gtgVjsLBVjpYzubZXzNNA27bhxT9yALuozzQZ09dUZvJhSRbgxU96y3Fyk+R5kLubM85o2rDu6aygOKGXN+7eDHIKvhimnZFtuLnK7CoBlMhk2/XOQxc9jypIQ5uQsd8LWaUPQztsdc0b2VCZjtnJzxuakQQgLML5isOoQoCkMWeJhZGQ7tcTRxoGYWGJiqfmje8PbwwWzHrRdwTd9C7uawt7yWYzBISSSvOiOPnj1vm7o1Fb89FpqPtwlOmOvX2hrPDdY/NTZUN8W2Pn6vcr7f06/B21bKYyeufjts7G4VlGJEF/DFmTU5rMJ0fhwyxksekx9DS9NvT5uLnKsezYWF4tvYPtfV/Bwn3b4OD3boPO8MeIOJK7ai0kDQ7HiT/FDJ3cEemL/W/c1yb9ydXZCZU2d6ONpY8044ukBoZj+/WEM0Zvgq5mlagAZiwEMSZ5MJsOLQ7vauhnNhj1+czNnk0L0TL811erEOzFx1V7EG5ijtXLinWipMP2tunGFXLEMXS+oq39LtHCVo0JTHRYAQ+/wx9A7xOWntfN2F53ndU93Pxx5ZxhaubkYFcAATdduAurXLXrpmwMYGRlkcDBlL/4vuj36dPBGiG8LbDlZaOvmmMx++niJSBLElnu3R/071w9FNJ5qO2VIZ3z5TIxFzz2kux9y54/AJ09FG7X/oG5t4SKXIaqj/orDtqBwlmN/8n22bgYAyxRpjOrog52v34v4ngE6t/tYxJpQlqApi0kmk6GrfyuHqeRr1FUsXboUISEhcHNzQ0xMDPbs2aN122PHjuGRRx5BSEgIZDIZFi9e3GSbd955BzKZTO0WFhbW9GBEZBdsPevJVJ3atsC214Zg39txao/PSAgzex6LMdVedfF0c8HR2fH47tlYsx7XnIwJch1tGvX9vQPx+nDTP8c0rYtF9UQHMOvWrUNSUhJmzZqF/fv3IyIiAvHx8Sgs1Nwddf36dXTq1Anz589HQID2iLVnz564fPmy8rZjxw6xTSNqNkb3rZ/N09+Ka+A4EkGoL8tvjRWcP3qiL/p08MaqxDsN3kd1OramRfcUznL9tXFsrKGH6LHoYBu3RNrMHQA7EtEDq4sWLcKkSZOQmJgIAFi+fDk2bNiAlStX4vXXX2+y/Z133ok776z/w9X0vLIhzs46Axwiuu1fo3rj3jA/DLJR4TopUs3dMWSasDFrLmk7zo/P3y1qH3dXOba8OhhyJ5ndd/ff3zsAG4/kY3ijsvPrJt+Fv69Xa1yY0hyMKdbWu7399Wboyynr3LYl/jx9VeQx7Tu4NRdRAUxVVRWysrIwc+ZM5WNOTk6Ii4tDRkaGSQ05ffo0goKC4ObmhtjYWKSkpKBDB83d1JWVlaisrFTeLy0tNencRFLj7irHA+HSrJRsCapv2D4tzPON1cvdBXvfjIPCRrVIpDKrbuH/ReDB8KAmwbSz3MliwYtYf06/BwWlN02aPq7L/NG98foPR5T3595a86qrGYLgafHdIXeS4YHwQJOPpcoRQhxRf5lXr15FbW0t/P3VM8j9/f2Rn59vdCNiYmKwevVqpKamYtmyZcjJycHAgQNRVlamcfuUlBR4eXkpb8HB7KIkonqxndrgucGd8cGYCP0b69G2lcIqw0xS5uHqjOG9A9HCDDOlLCW4tQeiQyy3EOHjKjlhkwd1wlO31ry6N8wP7z7cG+tfENcDp6qlwhlvP9ADfTqYlrTtLDc9ZNG19Ict2MVv3PDhw5X/Dw8PR0xMDDp27Ihvv/0WTz/9dJPtZ86ciaSkJOX90tJSBjFEIrR0s4s/fYuQyWRmSZ4kMpVMJsMTMbZPeF87+S6ThiJ/mToAvx65hBfvta9yFaLexXx9fSGXy1FQUKD2eEFBgVnzV7y9vdGtWzecOXNG4/MKhQIKhX10TRJJ0cL/i8DUr/fj+SG61yLSZmRkEL7Zk4dOJhY1sweDurWFj4cLerf31rld6xauuFZRZfBx/b34HiUVnY0Y6vGwgzXQDKVpyr2YPJne7b3sMn9IVADj6uqKqKgopKWlYdSoUQCAuro6pKWlYerUqWZrVHl5ObKzs/HUU0+Z7ZhEdFuobwtseGmg0fvf1akNtrw6GEEqq3JLiWr+ZwuFM/a+Gad3vaTfXxmEmHc3o05P7uiGlwagorIWfq0suwq1o+lrw7o2j0UH41pFFe7qZPgwU1f/Vpg0MBS+LdUDVXOuBE26ie5HTkpKwoQJExAdHY1+/fph8eLFqKioUM5KGj9+PNq1a4eUlBQA9Ym/x48fV/7/4sWLOHjwIFq2bIkuXeq//U2bNg0PPvggOnbsiEuXLmHWrFmQy+UYO3asua6TiMzMHpJMn7yrA77cnYdpw7rr3VbXN05DVmVu20qB+3r44/djBTq36xlkf99UxbLmZ/DWaUOwL/caRvdtb72TNiJ3kuGFe8T3Rr45wvR1kprJhCGLEB3AjBkzBleuXEFycjLy8/MRGRmJ1NRUZWJvXl4enJxuvxlcunQJffr0Ud5fuHAhFi5ciMGDByM9PR0AcOHCBYwdOxZFRUVo27YtBgwYgN27d6NtW04RJSLt5o7shaT7uqN1C1f9G5NdCvVtgVAHGIok6zMqk2/q1Klah4wagpIGISEhervU1q5da0wziKiZk8lkDF6Imin7rpBEREREpAEDGCIiIhtxc4DFUW3FcYtBEBGpCPB0Q4fWHnCRyyQ1BZYc06v3dcOB88W4r4e//o1JIwYwRNQsyJ1k2DptCIDms1YM2a8Xh2ouCufvqUBBaaXFV6F2hL8ADiERUbMhd5LprfdC9u3xO+urrr+sJQCwtchg0+rZfPtsLP5xdyj+Oz7KTC1yXOyBISIiyUgZ3RtvPdADLe1s7aUtrw7GkYsluL+3aVXpO7ZpgeQHTa8vo48jdELa128AERGRDjKZzO6CF6C+sKM9FHdsTjiERERE5EACvJrHMhYMYIiIiBzAN5Puwj3d22LxmEi4mrD6tFTYXz8cERHZVLCPh62bQEaI7dwGsZ3bKO9P7B+ClgpnuDhoMMMAhoiIAAD/ey4Wq3bm4s0Rd9i6KWQG7zzU09ZNsCgGMEREBACIDmmN6JDWtm4GkUEcs1+JiMgCXFn2nchuMIAhIjLQzOFhtm4CkVk4O0BejPSvgIjISoK83fHbywNt3Qwik93duQ36d26Df9wdauumGI05MEREItwR6IkPx/ZBkHfzqLVBjslZ7oSvJ91l62aYhAEMEZFID0YE2boJRM0eh5CIiIhIchjAEBERkeQwgCEiIiLJYQBDREREksMAhoiIiCSHAQwRERFJDgMYIiIikhwGMERERCQ5DGCIiIhIchjAEBERkeQwgCEiIiLJYQBDREREksMAhoiIiCTHIVajFgQBAFBaWmrjlhAREZGhGj63Gz7HxXCIAKasrAwAEBwcbOOWEBERkVhlZWXw8vIStY9MMCbssTN1dXW4dOkSWrVqBZlMZtZjl5aWIjg4GOfPn4enp6dZj20PHP36AMe/Rl6f9Dn6NTr69QGOf42Wuj5BEFBWVoagoCA4OYnLanGIHhgnJye0b9/eoufw9PR0yF/KBo5+fYDjXyOvT/oc/Rod/foAx79GS1yf2J6XBkziJSIiIslhAENERESSwwBGD4VCgVmzZkGhUNi6KRbh6NcHOP418vqkz9Gv0dGvD3D8a7TH63OIJF4iIiJqXtgDQ0RERJLDAIaIiIgkhwEMERERSQ4DGCIiIpIcBjB6LF26FCEhIXBzc0NMTAz27Nlj6yYhJSUFd955J1q1agU/Pz+MGjUKp06dUttmyJAhkMlkarfnnntObZu8vDyMGDECHh4e8PPzw2uvvYaamhq1bdLT09G3b18oFAp06dIFq1evbtIec/+M3nnnnSZtDwsLUz5/8+ZNvPDCC2jTpg1atmyJRx55BAUFBZK4NgAICQlpcn0ymQwvvPACAGm+dtu3b8eDDz6IoKAgyGQyrF+/Xu15QRCQnJyMwMBAuLu7Iy4uDqdPn1bb5tq1axg3bhw8PT3h7e2Np59+GuXl5WrbHD58GAMHDoSbmxuCg4OxYMGCJm357rvvEBYWBjc3N/Tu3RsbN24U3RYx11ddXY0ZM2agd+/eaNGiBYKCgjB+/HhcunRJ7RiaXvf58+fb/fUBwMSJE5u0PSEhQW0be379DLlGTX+TMpkM77//vnIbe34NDflcsKf3TkPaopdAWq1du1ZwdXUVVq5cKRw7dkyYNGmS4O3tLRQUFNi0XfHx8cKqVauEo0ePCgcPHhTuv/9+oUOHDkJ5eblym8GDBwuTJk0SLl++rLyVlJQon6+pqRF69eolxMXFCQcOHBA2btwo+Pr6CjNnzlRuc/bsWcHDw0NISkoSjh8/Lnz44YeCXC4XUlNTldtY4mc0a9YsoWfPnmptv3LlivL55557TggODhbS0tKEffv2CXfddZfQv39/SVybIAhCYWGh2rX98ccfAgBh69atgiBI87XbuHGj8Oabbwo//PCDAED48ccf1Z6fP3++4OXlJaxfv144dOiQ8NBDDwmhoaHCjRs3lNskJCQIERERwu7du4U///xT6NKlizB27Fjl8yUlJYK/v78wbtw44ejRo8I333wjuLu7C5988olym507dwpyuVxYsGCBcPz4ceGtt94SXFxchCNHjohqi5jrKy4uFuLi4oR169YJJ0+eFDIyMoR+/foJUVFRasfo2LGjMGfOHLXXVfVv1l6vTxAEYcKECUJCQoJa269du6a2jT2/foZco+q1Xb58WVi5cqUgk8mE7Oxs5Tb2/Boa8rlgT++d+tpiCAYwOvTr10944YUXlPdra2uFoKAgISUlxYataqqwsFAAIGzbtk352ODBg4WXX35Z6z4bN24UnJychPz8fOVjy5YtEzw9PYXKykpBEARh+vTpQs+ePdX2GzNmjBAfH6+8b4mf0axZs4SIiAiNzxUXFwsuLi7Cd999p3zsxIkTAgAhIyPD7q9Nk5dfflno3LmzUFdXJwiCtF87QRCafDjU1dUJAQEBwvvvv698rLi4WFAoFMI333wjCIIgHD9+XAAg7N27V7nNb7/9JshkMuHixYuCIAjCxx9/LPj4+CivURAEYcaMGUL37t2V9x977DFhxIgRau2JiYkRnn32WYPbIvb6NNmzZ48AQDh37pzysY4dOwoffPCB1n3s+fomTJggjBw5Uus+Unr9tF1jYyNHjhTuvfdetcek8hoKQtPPBXt67zSkLYbgEJIWVVVVyMrKQlxcnPIxJycnxMXFISMjw4Yta6qkpAQA0Lp1a7XHv/rqK/j6+qJXr16YOXMmrl+/rnwuIyMDvXv3hr+/v/Kx+Ph4lJaW4tixY8ptVK+/YZuG67fkz+j06dMICgpCp06dMG7cOOTl5QEAsrKyUF1drXbOsLAwdOjQQXlOe782VVVVVfjyyy/xj3/8Q20hUim/do3l5OQgPz9f7VxeXl6IiYlRe828vb0RHR2t3CYuLg5OTk7IzMxUbjNo0CC4urqqXdOpU6fw999/G3TdhrTFHEpKSiCTyeDt7a32+Pz589GmTRv06dMH77//vlrXvL1fX3p6Ovz8/NC9e3dMmTIFRUVFam13pNevoKAAGzZswNNPP93kOam8ho0/F+zpvdOQthjCIRZztISrV6+itrZW7YUEAH9/f5w8edJGrWqqrq4Or7zyCu6++2706tVL+fgTTzyBjh07IigoCIcPH8aMGTNw6tQp/PDDDwCA/Px8jdfW8JyubUpLS3Hjxg38/fffFvkZxcTEYPXq1ejevTsuX76M2bNnY+DAgTh69Cjy8/Ph6ura5IPB399fb7vt4doaW79+PYqLizFx4kTlY1J+7TRpaJOmc6m218/PT+15Z2dntG7dWm2b0NDQJsdoeM7Hx0frdaseQ19bTHXz5k3MmDEDY8eOVVv07qWXXkLfvn3RunVr7Nq1CzNnzsTly5exaNEiu7++hIQEjB49GqGhocjOzsYbb7yB4cOHIyMjA3K53KFePwD4/PPP0apVK4wePVrtcam8hpo+F+zpvdOQthiCAYzEvfDCCzh69Ch27Nih9vjkyZOV/+/duzcCAwMxdOhQZGdno3PnztZupijDhw9X/j88PBwxMTHo2LEjvv32W7i7u9uwZeb32WefYfjw4QgKClI+JuXXrrmrrq7GY489BkEQsGzZMrXnkpKSlP8PDw+Hq6srnn32WaSkpNhVeXZNHn/8ceX/e/fujfDwcHTu3Bnp6ekYOnSoDVtmGStXrsS4cePg5uam9rhUXkNtnwuOhkNIWvj6+kIulzfJii4oKEBAQICNWqVu6tSp+PXXX7F161a0b99e57YxMTEAgDNnzgAAAgICNF5bw3O6tvH09IS7u7vVfkbe3t7o1q0bzpw5g4CAAFRVVaG4uFjrOaVybefOncPmzZvxzDPP6NxOyq+dapt0nSsgIACFhYVqz9fU1ODatWtmeV1Vn9fXFmM1BC/nzp3DH3/8odb7oklMTAxqamqQm5urs+2q7bbl9anq1KkTfH191X4npf76Nfjzzz9x6tQpvX+XgH2+hto+F+zpvdOQthiCAYwWrq6uiIqKQlpamvKxuro6pKWlITY21oYtq59iN3XqVPz444/YsmVLky5LTQ4ePAgACAwMBADExsbiyJEjam86DW+6PXr0UG6jev0N2zRcv7V+RuXl5cjOzkZgYCCioqLg4uKids5Tp04hLy9PeU6pXNuqVavg5+eHESNG6NxOyq8dAISGhiIgIEDtXKWlpcjMzFR7zYqLi5GVlaXcZsuWLairq1MGcLGxsdi+fTuqq6vVrql79+7w8fEx6LoNaYsxGoKX06dPY/PmzWjTpo3efQ4ePAgnJyfl0Is9X19jFy5cQFFRkdrvpJRfP1WfffYZoqKiEBERoXdbe3oN9X0u2NN7pyFtMYjB6b7N0Nq1awWFQiGsXr1aOH78uDB58mTB29tbLUPbFqZMmSJ4eXkJ6enpatP5rl+/LgiCIJw5c0aYM2eOsG/fPiEnJ0f46aefhE6dOgmDBg1SHqNhutywYcOEgwcPCqmpqULbtm01Tpd77bXXhBMnTghLly7VOF3O3D+jV199VUhPTxdycnKEnTt3CnFxcYKvr69QWFgoCEL99LsOHToIW7ZsEfbt2yfExsYKsbGxkri2BrW1tUKHDh2EGTNmqD0u1deurKxMOHDggHDgwAEBgLBo0SLhwIEDylk48+fPF7y9vYWffvpJOHz4sDBy5EiN06j79OkjZGZmCjt27BC6du2qNg23uLhY8Pf3F5566inh6NGjwtq1awUPD48mU1SdnZ2FhQsXCidOnBBmzZqlcYqqvraIub6qqirhoYceEtq3by8cPHhQ7W+yYebGrl27hA8++EA4ePCgkJ2dLXz55ZdC27ZthfHjx9v99ZWVlQnTpk0TMjIyhJycHGHz5s1C3759ha5duwo3b96UxOun7xoblJSUCB4eHsKyZcua7G/vr6G+zwVBsK/3Tn1tMQQDGD0+/PBDoUOHDoKrq6vQr18/Yffu3bZukgBA423VqlWCIAhCXl6eMGjQIKF169aCQqEQunTpIrz22mtqtUQEQRByc3OF4cOHC+7u7oKvr6/w6quvCtXV1WrbbN26VYiMjBRcXV2FTp06Kc+hytw/ozFjxgiBgYGCq6ur0K5dO2HMmDHCmTNnlM/fuHFDeP755wUfHx/Bw8NDePjhh4XLly9L4toa/P777wIA4dSpU2qPS/W127p1q8bfyQkTJgiCUD819O233xb8/f0FhUIhDB06tMm1FxUVCWPHjhVatmwpeHp6ComJiUJZWZnaNocOHRIGDBggKBQKoV27dsL8+fObtOXbb78VunXrJri6ugo9e/YUNmzYoPa8IW0Rc305OTla/yYbavtkZWUJMTExgpeXl+Dm5ibccccdwrvvvqsWANjr9V2/fl0YNmyY0LZtW8HFxUXo2LGjMGnSpCaBrj2/fvquscEnn3wiuLu7C8XFxU32t/fXUN/ngiDY13unIW3RR3brwomIiIgkgzkwREREJDkMYIiIiEhyGMAQERGR5DCAISIiIslhAENERESSwwCGiIiIJIcBDBEREUkOAxgiIiKSHAYwREREJDkMYIiIiEhyGMAQERGR5DCAISIiIsn5fxswFj9F47YrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stepsi, losslog10i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "920ba531-9e33-4f00-9a12-9e8529b8a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to do the same thing for test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b13aa917-91f7-4841-af48-a0b2cb3a2a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 2.0668270587921143\n",
      "val loss : 2.104844808578491\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking, can also the context manager instead\n",
    "def split_loss(split):\n",
    "    ds = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val':   (Xva, Yva),\n",
    "        'test':  (Xte, Yte),\n",
    "    }\n",
    "    x,y = ds[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact_mean = hpreact.mean(dim=0, keepdim=True)\n",
    "    hpreact_std  = hpreact.std(dim=0, keepdim=True)\n",
    "    hpreact = bngain * (hpreact - hpreact_mean)/hpreact_std + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(f'{split} loss : {loss.item()}')\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8be2a03-36fe-448d-9839-684c695b5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch norm is not doing much here, since we have already initialised properly and the network is shallow\n",
    "# but with deeper neural nets, different types of connections, for eg. residual connections\n",
    "# it'll be very difficult to tune the scales of weight matrices so all activations throughout the NN is roughly gaussian\n",
    "# but it'll be very easy to sprinkle batch norm throughout the NN\n",
    "# in particular it's common to look at every single linear layer (or convolutions, linear arithmetic)\n",
    "# and append a batch norm layer right after it to control the scale of these activations that flows out of it the linear/conv. layer\n",
    "# we do this throughout the neural net, and this controls the activations throughout the neural net\n",
    "# we don't have to hand tune the mathematics and significantly stabilizes the training\n",
    "\n",
    "# The stability offered by batch norm COMES AT A TERRIBLE COST\n",
    "# Something terribly strange and unnatural is happening\n",
    "# Used to be that we used one example at a time\n",
    "# Because of efficiency of training we started to use batches of examples, but they were processed in parallel\n",
    "# But now with batch norm, because of norm. in a batch we introduce a coupling across these examples\n",
    "# mathematically in the forward and backward pass\n",
    "# So now, the hpreact and logits are not just a function of one example\n",
    "# but a function of all the other examples that happen to come for a ride in that batch, and they're sample randomly\n",
    "# Depending on what other samples come for a ride, h is going to change suddenly and jitter, if one can imagine sampling diff. examples\n",
    "# because the statistics of mean and std. dev are going to be affected\n",
    "# Jitter for h, jitter for logits\n",
    "# One would assume that this is bug, or undesired, but turns out this actually helps out with the NN training as a side effect\n",
    "# You could think of it as a regularizer based on other examples, sort of like padding with data augmentation\n",
    "\n",
    "# With this second order effect of regularizer, it has becomes harder to remove batch norm. \n",
    "# No one like the property that examples in the property are coupled mathematically and in the forward pass\n",
    "# People have tried to deprecate use of batch norm and move to other norm techniques that do not couple examples with batch\n",
    "# Examples are layer normalisation, instance normalisation, group normalisation and so on\n",
    "\n",
    "# But, batch norm was the first kind of normalisation to be introduced\n",
    "# works extremely well, happened to have this regularization effect, stablized the training\n",
    "# people have tried to move to other techniques, but it's been hard because it just works quiet well\n",
    "# quite effective at controlling activations, their distributions and the reg. effect\n",
    "\n",
    "# Another strange outcomes\n",
    "# Once we've trained a NN we'd like to be able to deploy it somewhere\n",
    "# feed it a single example and get a prediction out from our neural net\n",
    "# but how do we do that when the NN expects batches ??\n",
    "# proposal in the paper\n",
    "# have a step after training that calculates and sets the batch norm mean and std dev a single time over the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6afd3ed-e276-4dcb-bae2-326587c2dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate batch norm mean and std dev at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # calculate the bnmean bnstd\n",
    "    bnmean = hpreact.mean(dim=0, keepdim=True)\n",
    "    bnstd = hpreact.std(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "668174ca-0343-46dd-aa80-2b67304ca453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets use it at test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a46cc9f2-b893-4623-a34d-01c6cab80baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 2.0668270587921143\n",
      "val loss : 2.1049270629882812\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking, can also the context manager instead\n",
    "def split_loss(split):\n",
    "    ds = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val':   (Xva, Yva),\n",
    "        'test':  (Xte, Yte),\n",
    "    }\n",
    "    x,y = ds[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    #hpreact_mean = hpreact.mean(dim=0, keepdim=True)\n",
    "    #hpreact_std  = hpreact.std(dim=0, keepdim=True)\n",
    "    hpreact = bngain * (hpreact - bnmean)/bnstd + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(f'{split} loss : {loss.item()}')\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "393de0f7-e7ce-4152-9885-6e0761e58f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which means that now we can forward a single example\n",
    "# because the batch norm mean and std dev are fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66c4799e-e80d-435a-bbf9-49ca576ffd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having said that nobody wants to estimate these after the NN training is complete\n",
    "# One more idea introduced by the batch norm paper\n",
    "# we can estimate the mean and std dev in a running manner during training of NN\n",
    "# we estimate on the side while training\n",
    "# let's see what that looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bbc1cac-c1d8-4baa-a94d-e7f0424fd784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12297"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embed = 10   # dimensionality of characters in the embedding vector\n",
    "n_hidden = 200 # number of neurons in the hidden layer\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embed),             generator=g)\n",
    "W1 = torch.randn((block_size * n_embed, n_hidden), generator=g) * (5/3/(block_size*n_embed)**0.5)\n",
    "b1 = torch.randn(n_hidden,                         generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),           generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                       generator=g) * 0\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))  # we want the initial values to affect the hpreact in a nice gaussian\n",
    "bnbias = torch.zeros((1, n_hidden)) # we want the initial values to affect the hpreact in a nice gaussian\n",
    "\n",
    "bnmean_running = torch.zeros((1, n_hidden)) # the way we init W1 and b1, hpreact will be roughly unit gaussian, mean roughly zero\n",
    "bnstd_running = torch.ones((1, n_hidden))   # std dev roughly one\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "sum(p.nelement() for p in parameters) # total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "043c05c0-a7de-4de7-8121-41c38c693252",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94da9bb8-dd99-47d0-bb8b-5297107dfa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "losslog10i = []\n",
    "stepsi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9d251d4-4f07-4df8-bb2e-06dfa1a7ca43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0/200000: 3.3146889209747314\n",
      "Loss 20000/200000: 2.3374552726745605\n",
      "Loss 40000/200000: 2.011928081512451\n",
      "Loss 60000/200000: 2.4775009155273438\n",
      "Loss 80000/200000: 2.278811454772949\n",
      "Loss 100000/200000: 1.9473825693130493\n",
      "Loss 120000/200000: 1.9836552143096924\n",
      "Loss 140000/200000: 2.3839313983917236\n",
      "Loss 160000/200000: 1.9733015298843384\n",
      "Loss 180000/200000: 1.9972705841064453\n"
     ]
    }
   ],
   "source": [
    "tot_steps = len(stepsi)\n",
    "\n",
    "for i in range(max_steps):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]                         # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    hpreact = embcat @ W1 + b1          # hidden layer pre-activation\n",
    "    bnmeani = hpreact.mean(dim=0, keepdim=True)  # bn mean for ith iter        <-----\n",
    "    bnstdi  = hpreact.std(dim=0, keepdim=True)   # bn std  for ith iter        <----- \n",
    "    hpreact = bngain * (hpreact - bnmeani)/bnstdi + bnbias     # normalize hpreact, also scale and shift\n",
    "    \n",
    "    # not a part of gradient optimisation, keeping track of running values\n",
    "    with torch.no_grad():\n",
    "        # mostly what it used to be, a little of where it is headed right now\n",
    "        \n",
    "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "        bnstd_running  = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    \n",
    "    h = torch.tanh(hpreact)             # hidden layer\n",
    "    logits = h @ W2 + b2                # output layer\n",
    "    loss = F.cross_entropy(logits, Yb)  # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01    # switch learning rate\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    stepsi.append(tot_steps+i)\n",
    "    lossi.append(loss.item())\n",
    "    losslog10i.append(loss.log10().item())\n",
    "    \n",
    "    # Print loss \n",
    "    if (i)%(max_steps/10) == 0:\n",
    "        print(f\"Loss {i}/{max_steps}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "650c0a2c-ea36-4adc-b827-79dbe6890588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use it at test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e355d54-67b7-4e48-9763-5654b9f99ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 2.06659197807312\n",
      "val loss : 2.1050572395324707\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking, can also the context manager instead\n",
    "def split_loss(split):\n",
    "    ds = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val':   (Xva, Yva),\n",
    "        'test':  (Xte, Yte),\n",
    "    }\n",
    "    x,y = ds[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean_running)/bnstd_running + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(f'{split} loss : {loss.item()}')\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8519731-cfd6-4a9c-b55b-f52390c4638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch norm discussion more or less done, two more things\n",
    "# epsilon in the batch norm normalize equation is a tiny number that prevent division by zero, in case the variance is exactly zero\n",
    "# second thing we are being a bit wasteful\n",
    "#    hpreact = embcat @ W1 + b1          <------ CHECK THIS BIAS ADDED HERE\n",
    "#    bnmeani = hpreact.mean(dim=0, keepdim=True)\n",
    "#    bnstdi  = hpreact.std(dim=0, keepdim=True)\n",
    "#    hpreact = bngain * (hpreact - bnmeani)/bnstdi + bnbias     <--- BUT SUBTRACTED HERE\n",
    "# those biases are not doing anything\n",
    "# if you look at b1.grad it's going to be zero\n",
    "\n",
    "# Whenver you're using batch normalisation layer\n",
    "# if you have weight layers before (linear or conv)\n",
    "# better off not using bias at all for those layers\n",
    "# its the bnbias that's in charge of the bias instead of the b1\n",
    "# batch norm has it own bias, so there's no need to add bias to the weight layers before a bn layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b4a7df9-97bd-40a4-8510-0ca65c9df67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12097"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embed = 10   # dimensionality of characters in the embedding vector\n",
    "n_hidden = 200 # number of neurons in the hidden layer\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embed),             generator=g)\n",
    "W1 = torch.randn((block_size * n_embed, n_hidden), generator=g) * (5/3/(block_size*n_embed)**0.5)\n",
    "# b1 = torch.randn(n_hidden,                         generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),           generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                       generator=g) * 0\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))  # we want the initial values to affect the hpreact in a nice gaussian\n",
    "bnbias = torch.zeros((1, n_hidden)) # we want the initial values to affect the hpreact in a nice gaussian\n",
    "\n",
    "bnmean_running = torch.zeros((1, n_hidden)) # the way we init W1 and b1, hpreact will be roughly unit gaussian, mean roughly zero\n",
    "bnstd_running = torch.ones((1, n_hidden))   # std dev roughly one\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "sum(p.nelement() for p in parameters) # total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6011a112-1803-469e-9dd8-6e310230bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "799cec60-f545-44ff-a5e0-b766a1e4683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "losslog10i = []\n",
    "stepsi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ab0ad99-15eb-483b-a649-ef974891352a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0/200000: 3.3238625526428223\n",
      "Loss 20000/200000: 2.567518711090088\n",
      "Loss 40000/200000: 2.2446329593658447\n",
      "Loss 60000/200000: 2.0785391330718994\n",
      "Loss 80000/200000: 2.291823148727417\n",
      "Loss 100000/200000: 2.367293357849121\n",
      "Loss 120000/200000: 1.6413521766662598\n",
      "Loss 140000/200000: 2.2231295108795166\n",
      "Loss 160000/200000: 2.0996553897857666\n",
      "Loss 180000/200000: 2.0198497772216797\n"
     ]
    }
   ],
   "source": [
    "tot_steps = len(stepsi)\n",
    "\n",
    "for i in range(max_steps):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]                         # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    hpreact = embcat @ W1 # + b1          # hidden layer pre-activation\n",
    "    bnmeani = hpreact.mean(dim=0, keepdim=True)  # bn mean for ith iter        <-----\n",
    "    bnstdi  = hpreact.std(dim=0, keepdim=True)   # bn std  for ith iter        <----- \n",
    "    hpreact = bngain * (hpreact - bnmeani)/bnstdi + bnbias     # normalize hpreact, also scale and shift\n",
    "    \n",
    "    # not a part of gradient optimisation, keeping track of running values\n",
    "    with torch.no_grad():\n",
    "        # mostly what it used to be, a little of where it is headed right now\n",
    "        \n",
    "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "        bnstd_running  = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    \n",
    "    h = torch.tanh(hpreact)             # hidden layer\n",
    "    logits = h @ W2 + b2                # output layer\n",
    "    loss = F.cross_entropy(logits, Yb)  # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01    # switch learning rate\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    stepsi.append(tot_steps+i)\n",
    "    lossi.append(loss.item())\n",
    "    losslog10i.append(loss.log10().item())\n",
    "    \n",
    "    # Print loss \n",
    "    if (i)%(max_steps/10) == 0:\n",
    "        print(f\"Loss {i}/{max_steps}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21b6f39e-e849-47b5-8d41-e40e08b0428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally ets use it at test time without the bias b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3cc9294-5d49-4b89-a165-1d549911c4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 2.0674147605895996\n",
      "val loss : 2.1056840419769287\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking, can also the context manager instead\n",
    "def split_loss(split):\n",
    "    ds = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val':   (Xva, Yva),\n",
    "        'test':  (Xte, Yte),\n",
    "    }\n",
    "    x,y = ds[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 # + b1\n",
    "    hpreact = bngain * (hpreact - bnmean_running)/bnstd_running + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(f'{split} loss : {loss.item()}')\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e9c7b15-4c1b-4a32-9ec0-7dd212fb73c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just sample for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "759443ac-8860-40d8-8c05-7b60f7a2d097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlah.\n",
      "amorie.\n",
      "khi.\n",
      "mri.\n",
      "reity.\n",
      "salaysie.\n",
      "mahnen.\n",
      "delynn.\n",
      "jareei.\n",
      "ner.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    block_size = 3\n",
    "    context = [0] * block_size # initialize all with ...\n",
    "    while True:\n",
    "        # forward pass\n",
    "        emb = C[torch.tensor([context])] # (1, block_size, d)\n",
    "        embcat = emb.view(1, -1)\n",
    "        hpreact = embcat @ W1\n",
    "        hpreact = bngain * (hpreact - bnmean_running)/bnstd_running + bnbias\n",
    "        h = torch.tanh(hpreact)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1) # exponentiates the logits and then softmaxes them, similar to cross entroy no overflows\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # break if we reach the special token '.'\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "737b67e3-1687-45b2-9797-d4c49196178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note our losses going down with tricks\n",
    "\n",
    "# Train test validation split - in the beginning\n",
    "# Train Loss : 2.2589\n",
    "# Val Loss   : 2.2702\n",
    "\n",
    "# After increasing NN capacity and embedding size\n",
    "# Train Loss : 2.1190\n",
    "# Val Loss   : 2.1711\n",
    "\n",
    "# After fixing init weight bias softmax confidently wrong\n",
    "# Train loss : 2.0695\n",
    "# Val loss   : 2.1310\n",
    "\n",
    "# After fixing tanh saturation\n",
    "# train loss : 2.0355\n",
    "# val loss   : 2.1026\n",
    "\n",
    "# Now, with kaiming init and not using magic numbers for tanh saturation\n",
    "# train loss : 2.0376\n",
    "# val loss   : 2.1069\n",
    "\n",
    "# With batch norm\n",
    "# train loss : 2.0668\n",
    "# val loss   : 2.1048"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
