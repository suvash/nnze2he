{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0495e940-5955-4fe4-8a83-5b29ae38e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/karpathy/nanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8959712d-b067-4c43-a195-4f57658f5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two files of interest\n",
    "\n",
    "# train.py\n",
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c908b8cb-c38b-4a5a-b24a-30eba79e66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py is basically the same as gpt.py as we worked on\n",
    "\n",
    "# more complicated because we have save checkpoints\n",
    "# load pretrained weights\n",
    "# decaying the learning rate\n",
    "# compiling the model\n",
    "# using distributed training across multiple gpus\n",
    "# more options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb16357e-2f90-489a-b5ce-eb9f0257a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py SHOULD look pretty much similar\n",
    "\n",
    "# almost identical\n",
    "\n",
    "# except the multi headed attention which is implemented as a bunch of attention heads concatenated\n",
    "# but in the nanogpt it's built in a batched manner, meaning 4 dimensions, heads get their own dimension\n",
    "# lot more efficient since the the heads are treated as a batch dimension as well\n",
    "# mathematically equivalent though\n",
    "\n",
    "# gelu instead of relu\n",
    "\n",
    "# rest is the same\n",
    "\n",
    "# parameters in gpt are being loaded\n",
    "# some of them are weight decayed, some of them not\n",
    "\n",
    "# generation is very similar too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c9f5200-3199-4139-b221-743da5c9e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would it look like if we wanted to train GPT ourselves and how does it relate to what we've learnt today\n",
    "\n",
    "# Pretraining stage\n",
    "# training on lots of text and then babble\n",
    "# we trained a baby transformer (10M parameters)\n",
    "# our dataset is roughly 1M characters, 1M tokens\n",
    "\n",
    "# but openai uses sub work tokenization\n",
    "# our dataset would probably be 300,000 tokens\n",
    "\n",
    "# if you check the gpt3 paper\n",
    "# number of transformers of different sizes\n",
    "# biggest transformer has 175B parameters (ours is 10M, so it's 17500 times bigger than ours)\n",
    "# trained on 300B tokens (about a million fold increase)\n",
    "# by today's standard not even that large, 1T and above tokens is getting common\n",
    "\n",
    "# it's a massive infrastucture challenge\n",
    "\n",
    "# Now that's just pretraining stage\n",
    "# you only get a document completer at that point\n",
    "# not something that can answer your questions etc\n",
    "# probably will just complete your question to give more questions\n",
    "\n",
    "# next stage fine tuning\n",
    "# Fine tuning stage\n",
    "# Roughly 3 steps\n",
    "\n",
    "# Specialised QA dataset\n",
    "# model is fine tuned on these documents\n",
    "# so they start to reply to questions with answers after fine tuning\n",
    "# but that's just step 1\n",
    "\n",
    "# step 2\n",
    "# let the model respond with samples\n",
    "# then let expert rate the answers\n",
    "# use those rating to train a reward model\n",
    "# use that model to figure out the response to how much it would be desirable\n",
    "\n",
    "# step 3\n",
    "# once the reward model is there\n",
    "# use a PPO policy gradient reinforcement learning optimizer\n",
    "# fine tune the sampling policy so that\n",
    "# answers generated are expected to score a high reward according to the reward model\n",
    "\n",
    "# these steps take the model from being a document completer when sampled\n",
    "# to a high quality answer sampling completed, question answer complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd3f205-da01-41ee-a6ab-893ef0b56acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
