{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2eb2e79-bef2-4944-b2ed-aa4ca2fdf74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d6bfc7f-67fb-4c3d-b4bb-c8ecc41eb6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # Batch, Time, Channels : Time component being the given x tokens predict next token which moves over time\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e66a9d-b2a2-4c93-8146-b330cbdd0598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1 : nested for loop select previous and mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "871b17f1-01dd-442c-8ef6-9a3dfa3458db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want x[b, t] = mean_{i<=t} x[b, i]\n",
    "xbow = torch.zeros(B, T, C) # bow = bag of words : term used when averaging up words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36609fb-443d-45f9-a83a-d3c6707fc6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d301a8-ec0e-4a26-9d86-ab1500b33e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2 : lower triangular normalised over rows matrix multiply\n",
    "\n",
    "# we can very very efficient about this using matrix multiplication\n",
    "# lets look at a toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ebff4c-632c-4c83-b535-18ebbce1abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "wei\n",
    "\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C) => (B, T, T) @ (B, T, C) => batched matrix mult => (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2448a4b-c20f-414d-a60b-bd319366aba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf3b125c-c22a-4608-97d2-5d0ea208c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3 : using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79ecb652-caf0-401c-b2c0-03775ce3454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei  = torch.zeros(T, T) # torch.ones(T, T) : doesn't matter as long as all elments equal since they'll get softmaxed\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "\n",
    "xbow3 = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91413bb1-cdcd-421e-aa34-8c2c3183e83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow3, xbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc0e62c1-5d20-402f-9414-a35349704cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long story short\n",
    "# You can do weighted aggeration of past elements\n",
    "# by using matrix multiplication\n",
    "# of a lower triangular fashion\n",
    "# the elements of which are telling you how much of each elements fused/contributes to an individual position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "305e6347-7369-4025-b1bf-36d13231b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 4 : self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2685a8a6-01bf-4776-bbea-337d0fdb7a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to implement self attention for a small individual head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9be387ee-fe05-46bf-8c05-9bc16baf800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # Using 32 channels 4 x 8 tokens, each token 32 dimensional\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f01b543a-90ed-4248-9326-0658f5e6700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei  = torch.zeros(T, T) # torch.ones(T, T) : doesn't matter as long as all elments equal since they'll get softmaxed\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "\n",
    "out = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a008105-195b-4fa8-a732-f6cf816ae1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dffefd2e-2b72-4f66-8d7e-e9e76d7b31c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39cc776a-f634-4789-b216-cbb9ebc3b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in version 3 when we initialise the affinities to be zeroes,\n",
    "# then we see that wei gets uniform numbers, which softmax processes into a average\n",
    "\n",
    "# we don't want this to be uniform tokens\n",
    "# different tokens will find different other tokes more or less interesting, not uniformly interesting\n",
    "# we want that to be data dependent\n",
    "\n",
    "# this is the problem that self attention solves, information flow from the past to me in a data dependent way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e78e4c43-855f-4377-854c-b163c63f10e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEART OF THE THING\n",
    "\n",
    "# Every single token at each position will emit 2 vectors (emit ?? : say bring along with them I guess)\n",
    "# a Query vector and a Key vector (every token at every position)\n",
    "# Query vector : \"What am I looking for?\"\n",
    "# Key vector:    \"What do I contain?\"\n",
    "\n",
    "# The way we get affinities between these tokens in a sequence\n",
    "# we do a dot product between the Keys and the Queries\n",
    "# for token N, Query(N) will dot product with all the Key(s) of (0..N-1)\n",
    "# my query will dot product with all the keys of the other tokens (in the past)\n",
    "# and that dot product now becomes the weight (as in the wei)\n",
    "# instead of starting out normalised, the weights are deduced form Query Key dot product\n",
    "\n",
    "# If the key and query are aligned, they'll interact with high amount\n",
    "# and I will get to learn more about that specific token, as opposed to any other token in the sequence\n",
    "# that's it really"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "046d04f4-3bea-49bb-85b1-438f25275364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement single 'head' of self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c2952b6-f9bc-42fb-a789-4048d3aa1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # Using 32 channels 4 x 8 tokens, each token 32 dimensional\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3dbc7c0-c5d4-4394-9bab-1f7d00025ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single head of self attention\n",
    "# Remember C is channels, dimension of tokens\n",
    "\n",
    "head_size = 16 # attention hyper parm\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4fd9340-7641-40c4-9210-13e070718c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take note that the key and query production forward is in parallel\n",
    "\n",
    "k = key(x)   # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "\n",
    "# no communication has happened yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ddd146b-4b46-4a24-abfd-b7ee1ff34d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 16]), torch.Size([4, 8, 16]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape, q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bb0afa8-fb9d-4a9b-a1c7-b83f8cc0e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all queries now get dot producted with all the keys to produce the weights matrix\n",
    "\n",
    "# but we need to align the shapes first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18ce8bc2-d279-4b2a-9305-4efa50c05421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to transpose the last two dimensions, i.e. the last two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4acc6951-70c5-4553-9bc2-5c392b6d3c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-2, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5819d5d8-8680-4fe1-9f3e-706c9b7b08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multid dim matmul happen on last two dims, need to align the keys and queries in those dims, keep the batch outside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa254165-a848-4ebf-b75f-b0b5874a5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = q @ k.transpose(-2, -1) # (B, T, hs) @ (B, hs, T) ----> (B, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c6212a8-d0d0-420b-a8c3-70e83a179b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
       "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
       "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
       "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
       "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
       "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
       "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
       "        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4e74028-a44a-4a00-93fb-4fcaddcff419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every row of b(atch), we will have a T-square matrix giving us the affinities and these are the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "185c9078-b656-4fbb-b5be-424660afec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei  = torch.zeros(T, T) # torch.ones(T, T) # replaced by actual weights above\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "\n",
    "out = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9977618d-e01a-4dba-af94-e5c8a3458575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0248, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0052, 0.0091, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0521, 0.0135, 0.2482, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3171, 0.0214, 0.1642, 0.1188, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0412, 0.0487, 0.1046, 0.0742, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1060, 0.5347, 0.2059, 0.1030, 0.7402, 0.0192, 0.0000, 0.0000],\n",
       "        [0.4298, 0.3409, 0.1769, 0.2027, 0.0480, 0.8472, 0.2329, 0.0000],\n",
       "        [0.0238, 0.0316, 0.1002, 0.5013, 0.0117, 0.1336, 0.7671, 1.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce2ed848-f13c-41fa-b5d5-a5c6a8b896ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before wei was a constant value for each batch row, but now they're different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5312b673-47cd-4555-9997-57c1cfa9915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# say the 8th token, it know what content it has\n",
    "# and it knows what the position it has (token emb + pos emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d69cba84-0cf0-4d64-8a94-4d5bd9b96de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the token creates a query, \"i'm looking for this kind of stuff\"\n",
    "# other token before it get to influence the interaction by their keys\n",
    "# high affinity when the dot product operation gets a high return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad42604e-edf1-4687-ba73-5548d03d1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ah aw, that's not it, we emit one more value per token, that we call 'value', similar to key and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38b6445f-b488-4707-a714-6107db964b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # Using 32 channels 4 x 8 tokens, each token 32 dimensional\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb16dd0b-3d9f-4e31-b173-527256b96b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single head of self attention\n",
    "# Remember C is channels, dimension of tokens\n",
    "\n",
    "head_size = 16 # attention hyper parm\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "value = nn.Linear(C, head_size, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6cb4788-fa2a-4150-9fac-830e1c5e7023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and similarly we forward the value too, also in parallel\n",
    "k = key(x)   # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "\n",
    "v = value(x) # (B, T, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe96680c-3a26-4fc8-aa7b-fe016c58f32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get the weights\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, hs) @ (B, hs, T) ----> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94fdc8f4-617b-4da3-9e0e-1ec1d075754f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 8]), torch.Size([4, 8, 16]), torch.Size([4, 8, 32]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.shape, v.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb6841e8-1e66-45ce-b7a5-5740e82b080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but finally we don't just get the out directly from weights and x, instead we use the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87aa3876-0ce4-403a-bf68-70ecb4597138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d37c048-a99f-4af2-9bc8-9137949ca282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the output is head_size dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80b7ef19-b560-4278-9d6b-cdd72a2cce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# think of x as private information to the particular token, and v as the value communicated to other tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c659821-5f2a-427c-b955-52997d3e9111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query : here's what i'm interested in\n",
    "# key   : here's what I have\n",
    "# value : if you find we interesting(given the above affinities), here's what i'll communicate to you (you will have access to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "225e5d1c-7d51-42dc-919d-506d987b7844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query and keys act like filters which will define how much of value will be communicated after aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb56be47-c97b-45c7-9054-ccab1aaf9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "\n",
    "# Attention is a communication mechanism\n",
    "# number of nodes in DAG\n",
    "# every node has some vector of information (value)\n",
    "# gets to aggregate vectors of information via weighted sum from all of the nodes that point to it\n",
    "# aggregation done in a data dependent manner\n",
    "\n",
    "# No notion of space\n",
    "# spatial information has to be encoded, the nodes themselves don't know\n",
    "# unlike convolutions\n",
    "\n",
    "# Each example across batch dimension is processed independently and never talk to each other\n",
    "\n",
    "# Not being able to attend to future tokens does not necessarily have to be the case\n",
    "# In an encoder just remove the tril based masking, this model is effectively like a decoder hence the constraint\n",
    "# Attention mechanism does not care\n",
    "\n",
    "# Cross attention vs. Self attention\n",
    "# this one is self attention because the keys queries and values all come from the same source (from input x)\n",
    "# for eg. in Encoder Decoder Transformer\n",
    "# queries ar produced from x\n",
    "# keys and values can come from a whole different source(from the encoder block)\n",
    "# the source has the context we want to condition our attention on\n",
    "# When this happens its called cross attention\n",
    "# separate source of information we'd like to pool attention from into our nodes\n",
    "# self attention if we have nodes that'd like to look at each other and talk to each other\n",
    "\n",
    "# From the equation of self attention we're missing one more thing, dividing by sq.root of the head size\n",
    "# called as the scale attention\n",
    "# an important normalisation, since we've ad hoc applied head size, we need to turn that back down\n",
    "# in a sense we've let attention aggregate information over head_size dimensions\n",
    "# if q and k are gaussian, they matmul, they're outputs will be in the order of head_size\n",
    "# we need that to be guassian, so we normalise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd8f560d-74ac-4af5-b32d-25469825c434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.4690)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "_w = q @ k.transpose(-2, -1)\n",
    "\n",
    "_w.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b802ccf-f376-464f-a6e7-4954e9eff5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but if we normalise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d85c7e75-7971-4612-8bbd-10edfd5d689a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9957)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "_w = q @ k.transpose(-2, -1) * (head_size ** -0.5)\n",
    "\n",
    "_w.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8610c5f-4dd0-40ea-8c10-97a0470b9470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's back to gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d914e2c1-afd7-429d-a5e6-0c3953e9a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why is it important ?\n",
    "# wei is fed into softmax, and it's important that wei be fairly diffused at init time\n",
    "# if the values in wei are extreme, then the output of softmax can converge to be one hot vectors, not very useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4bc7530-166e-402d-8813-dd319530106f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5 ]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca94db3e-1f8f-4b80-92d1-d2c5a8952044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0228, 0.0015, 0.1382, 0.0015, 0.8359])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5 ])*9, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36c32be0-0989-42ef-806f-e03002a98f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax sharpens towards the max\n",
    "# therefore we don't want these values to be too extreme, esp. at init, otherwise softmax will be way to picky\n",
    "# every node will aggregate information only from other single node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6804f8a2-9e3a-438e-b0db-e7651f6b955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the above mechanism goes in a single 'head' of attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "053ecb0b-ac02-4b9e-92a6-6c5a5e205a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single head of self attention\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        # in Pytorch convention a variable that's not a parameter of the model is called a buffer\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        # emit keys and queries for x\n",
    "        k = self.key(x)  # (B, T, hs)\n",
    "        q = self.query(x) # (B, T, hs)\n",
    "        # compute attention\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "34d36547-ae52-4b21-8dfe-c5dda30898fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi head attention\n",
    "#\n",
    "# applying multiple attention heads in parallel and the concatenating the results\n",
    "# that's it really : the head sizes are matched so that the concatenation gives out the same dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c71c7b25-5376-4dca-985b-1913c1e1c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "99287efe-704c-4908-8e5a-aac20afeb358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward mechanism\n",
    "\n",
    "# is just a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e7bde11e-2ab5-4b3d-80a4-b143886a738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \" simple linear layer followed by non linearity \"\n",
    "    \n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe806006-63af-40b0-9336-8bd2e00cb38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tokens get an opportunity to use the multi headed attention, but then go right away into calculating logits\n",
    "# looked at each other but didn't have enough time to think on what they found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1653058c-deb7-4b59-a84f-5f55e7a0eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we (multi) self attend - this is the communication\n",
    "# then we feedforward - per token in parallel - independent of other tokens - now they need to think/react on that data individually\n",
    "# basically provide the ability to decide how much to fire based on the communicated and collected information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dc747b74-6cd0-42c1-a19f-4746e9c9cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we start to intersperse tho communication with the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d56622ef-057a-4040-b1fb-aea6ca6640fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" a transformer block : communication then computation \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)   # communication\n",
    "        x = self.ffwd(x) # computation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "95fe9d4b-e087-4f4e-b438-6342b2752869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the addition of these blocks, the network is starting to get pretty deep\n",
    "# but we are not yet getting good result\n",
    "# so we will visit one more idea from the attention paper\n",
    "# residual connections also called skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1280f1a2-03e8-4a93-9526-32278c8f9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basic idea is that\n",
    "# there's the residual pathway (without computation)\n",
    "# you're free to fork off the pathway and perform some more computation and project back to the pathway (via addition)\n",
    "# remember addition distributes gradients equally to the branches during backprop\n",
    "# gradient 'superhighway' all the way from supervision all the way to the input uninterrputed\n",
    "# residual blocks contribute very little in the beginning given how they're init'd\n",
    "# during learning they start to contribute more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9fbbc30-112e-4879-bf18-b7d82f2c030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" a transformer block : communication then computation \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(x)   # RESIDUAL ADD\n",
    "        x = x + self.ffwd(x) # RESIDUAL ADD\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2f1246f1-fa68-4c7d-99ef-ddb46deea088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out) # outcome of the linear layer to project back into the residual pathway\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c2990987-4c36-476d-a4bb-a7724b1af6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \" simple linear layer followed by non linearity \"\n",
    "    \n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed), # as mentioned in the paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed) # projection layer : the final projection back into the residual pathway\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f7a5b-1890-489d-bc1d-5bd0a0a61ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
